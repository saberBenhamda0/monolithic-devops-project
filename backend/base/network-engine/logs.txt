
==> Audit <==
|---------|--------------------------|----------|-----------------------|---------|---------------------|---------------------|
| Command |           Args           | Profile  |         User          | Version |     Start Time      |      End Time       |
|---------|--------------------------|----------|-----------------------|---------|---------------------|---------------------|
| start   | --driver=docker          | minikube | DESKTOP-7G3EBEF\vodka | v1.35.0 | 01 Dec 25 16:13 +01 | 01 Dec 25 16:14 +01 |
| stop    |                          | minikube | DESKTOP-7G3EBEF\vodka | v1.35.0 | 01 Dec 25 16:20 +01 | 01 Dec 25 16:20 +01 |
| start   |                          | minikube | DESKTOP-7G3EBEF\vodka | v1.35.0 | 01 Dec 25 16:20 +01 | 01 Dec 25 16:20 +01 |
| start   |                          | minikube | DESKTOP-7G3EBEF\vodka | v1.35.0 | 02 Dec 25 15:55 +01 | 02 Dec 25 15:55 +01 |
| image   | load manga2you_k8:latest | minikube | DESKTOP-7G3EBEF\vodka | v1.35.0 | 02 Dec 25 17:18 +01 | 02 Dec 25 17:20 +01 |
| image   | ls                       | minikube | DESKTOP-7G3EBEF\vodka | v1.35.0 | 02 Dec 25 17:22 +01 | 02 Dec 25 17:22 +01 |
| ip      |                          | minikube | DESKTOP-7G3EBEF\vodka | v1.35.0 | 02 Dec 25 18:10 +01 | 02 Dec 25 18:10 +01 |
| ip      |                          | minikube | DESKTOP-7G3EBEF\vodka | v1.35.0 | 02 Dec 25 18:15 +01 | 02 Dec 25 18:15 +01 |
| ip      |                          | minikube | DESKTOP-7G3EBEF\vodka | v1.35.0 | 02 Dec 25 18:16 +01 | 02 Dec 25 18:16 +01 |
| ip      |                          | minikube | DESKTOP-7G3EBEF\vodka | v1.35.0 | 02 Dec 25 18:27 +01 | 02 Dec 25 18:27 +01 |
| addons  | enable metrics-server    | minikube | DESKTOP-7G3EBEF\vodka | v1.35.0 | 02 Dec 25 19:51 +01 | 02 Dec 25 19:51 +01 |
| addons  | enable metrics-server    | minikube | DESKTOP-7G3EBEF\vodka | v1.35.0 | 02 Dec 25 19:52 +01 | 02 Dec 25 19:53 +01 |
| kubectl | top nodes                | minikube | DESKTOP-7G3EBEF\vodka | v1.35.0 | 02 Dec 25 20:09 +01 |                     |
| kubectl | top nodes                | minikube | DESKTOP-7G3EBEF\vodka | v1.35.0 | 02 Dec 25 20:10 +01 |                     |
| addons  | enable calico            | minikube | DESKTOP-7G3EBEF\vodka | v1.35.0 | 02 Dec 25 22:15 +01 |                     |
| addons  | enable calico            | minikube | DESKTOP-7G3EBEF\vodka | v1.35.0 | 02 Dec 25 22:25 +01 |                     |
|---------|--------------------------|----------|-----------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/12/02 15:55:06
Running on machine: DESKTOP-7G3EBEF
Binary: Built with gc go1.23.4 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1202 15:55:06.994792   11180 out.go:345] Setting OutFile to fd 92 ...
I1202 15:55:06.997257   11180 out.go:358] Setting ErrFile to fd 96...
W1202 15:55:07.023389   11180 root.go:314] Error reading config file at C:\Users\vodka\.minikube\config\config.json: open C:\Users\vodka\.minikube\config\config.json: The system cannot find the file specified.
I1202 15:55:07.054920   11180 out.go:352] Setting JSON to false
I1202 15:55:07.060646   11180 start.go:129] hostinfo: {"hostname":"DESKTOP-7G3EBEF","uptime":267057,"bootTime":1764420249,"procs":282,"os":"windows","platform":"Microsoft Windows 10 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.6466 Build 19045.6466","kernelVersion":"10.0.19045.6466 Build 19045.6466","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"2969c0ad-b726-4d39-98e9-7965c63c9936"}
W1202 15:55:07.065234   11180 start.go:137] gopshost.Virtualization returned error: not implemented yet
I1202 15:55:07.070178   11180 out.go:177] üòÑ  minikube v1.35.0 on Microsoft Windows 10 Pro 10.0.19045.6466 Build 19045.6466
I1202 15:55:07.074937   11180 notify.go:220] Checking for updates...
I1202 15:55:07.100792   11180 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I1202 15:55:07.106276   11180 driver.go:394] Setting default libvirt URI to qemu:///system
I1202 15:55:07.327834   11180 docker.go:123] docker version: linux-27.0.3:Docker Desktop 4.32.0 (157355)
I1202 15:55:07.369784   11180 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1202 15:55:09.824375   11180 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.4545913s)
I1202 15:55:09.829127   11180 info.go:266] docker info: {ID:7ce0231d-37a6-4462-99ad-db5f77824acd Containers:43 ContainersRunning:2 ContainersPaused:0 ContainersStopped:41 Images:34 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:63 OomKillDisable:true NGoroutines:77 SystemTime:2025-12-02 14:55:09.789768189 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8273297408 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae71819c4f5e67bb4d5ae76a6b735f29cc25774e Expected:ae71819c4f5e67bb4d5ae76a6b735f29cc25774e} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.15.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.28.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.32] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.14] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.10.0]] Warnings:<nil>}}
I1202 15:55:09.832772   11180 out.go:177] ‚ú®  Using the docker driver based on existing profile
I1202 15:55:09.835369   11180 start.go:297] selected driver: docker
I1202 15:55:09.835369   11180 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\vodka:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1202 15:55:09.835513   11180 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1202 15:55:09.893854   11180 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1202 15:55:10.335309   11180 info.go:266] docker info: {ID:7ce0231d-37a6-4462-99ad-db5f77824acd Containers:43 ContainersRunning:2 ContainersPaused:0 ContainersStopped:41 Images:34 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:63 OomKillDisable:true NGoroutines:77 SystemTime:2025-12-02 14:55:10.306389336 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8273297408 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae71819c4f5e67bb4d5ae76a6b735f29cc25774e Expected:ae71819c4f5e67bb4d5ae76a6b735f29cc25774e} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.15.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.28.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.32] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.14] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.10.0]] Warnings:<nil>}}
I1202 15:55:10.426073   11180 cni.go:84] Creating CNI manager for ""
I1202 15:55:10.426073   11180 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1202 15:55:10.426073   11180 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\vodka:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1202 15:55:10.429246   11180 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I1202 15:55:10.433334   11180 cache.go:121] Beginning downloading kic base image for docker with docker
I1202 15:55:10.435226   11180 out.go:177] üöú  Pulling base image v0.0.46 ...
I1202 15:55:10.435226   11180 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I1202 15:55:10.435226   11180 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I1202 15:55:10.435226   11180 preload.go:146] Found local preload: C:\Users\vodka\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I1202 15:55:10.435226   11180 cache.go:56] Caching tarball of preloaded images
I1202 15:55:10.435226   11180 preload.go:172] Found C:\Users\vodka\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1202 15:55:10.435226   11180 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I1202 15:55:10.435226   11180 profile.go:143] Saving config to C:\Users\vodka\.minikube\profiles\minikube\config.json ...
I1202 15:55:10.652719   11180 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I1202 15:55:10.653293   11180 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
I1202 15:55:10.653293   11180 cache.go:227] Successfully downloaded all kic artifacts
I1202 15:55:10.655393   11180 start.go:360] acquireMachinesLock for minikube: {Name:mkc334b56881bc29f3ed07caa4cf821ab90c46d2 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1202 15:55:10.655393   11180 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I1202 15:55:10.655949   11180 start.go:96] Skipping create...Using existing machine configuration
I1202 15:55:10.655949   11180 fix.go:54] fixHost starting: 
I1202 15:55:10.735859   11180 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1202 15:55:10.834139   11180 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1202 15:55:10.834540   11180 fix.go:138] unexpected machine state, will restart: <nil>
I1202 15:55:10.837692   11180 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I1202 15:55:10.871238   11180 cli_runner.go:164] Run: docker start minikube
I1202 15:55:11.679216   11180 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1202 15:55:11.816799   11180 kic.go:430] container "minikube" state is running.
I1202 15:55:11.884322   11180 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1202 15:55:12.023506   11180 profile.go:143] Saving config to C:\Users\vodka\.minikube\profiles\minikube\config.json ...
I1202 15:55:12.028894   11180 machine.go:93] provisionDockerMachine start ...
I1202 15:55:12.079773   11180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 15:55:12.248100   11180 main.go:141] libmachine: Using SSH client type: native
I1202 15:55:12.276001   11180 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1365360] 0x1367ea0 <nil>  [] 0s} 127.0.0.1 50568 <nil> <nil>}
I1202 15:55:12.276001   11180 main.go:141] libmachine: About to run SSH command:
hostname
I1202 15:55:12.279610   11180 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1202 15:55:15.502676   11180 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1202 15:55:15.503306   11180 ubuntu.go:169] provisioning hostname "minikube"
I1202 15:55:15.537276   11180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 15:55:15.664221   11180 main.go:141] libmachine: Using SSH client type: native
I1202 15:55:15.664945   11180 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1365360] 0x1367ea0 <nil>  [] 0s} 127.0.0.1 50568 <nil> <nil>}
I1202 15:55:15.664945   11180 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1202 15:55:15.955239   11180 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1202 15:55:15.984835   11180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 15:55:16.116094   11180 main.go:141] libmachine: Using SSH client type: native
I1202 15:55:16.116094   11180 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1365360] 0x1367ea0 <nil>  [] 0s} 127.0.0.1 50568 <nil> <nil>}
I1202 15:55:16.116094   11180 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1202 15:55:16.330962   11180 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1202 15:55:16.330962   11180 ubuntu.go:175] set auth options {CertDir:C:\Users\vodka\.minikube CaCertPath:C:\Users\vodka\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\vodka\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\vodka\.minikube\machines\server.pem ServerKeyPath:C:\Users\vodka\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\vodka\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\vodka\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\vodka\.minikube}
I1202 15:55:16.330962   11180 ubuntu.go:177] setting up certificates
I1202 15:55:16.330962   11180 provision.go:84] configureAuth start
I1202 15:55:16.363830   11180 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1202 15:55:16.477327   11180 provision.go:143] copyHostCerts
I1202 15:55:16.497153   11180 exec_runner.go:144] found C:\Users\vodka\.minikube/cert.pem, removing ...
I1202 15:55:16.497949   11180 exec_runner.go:203] rm: C:\Users\vodka\.minikube\cert.pem
I1202 15:55:16.498567   11180 exec_runner.go:151] cp: C:\Users\vodka\.minikube\certs\cert.pem --> C:\Users\vodka\.minikube/cert.pem (1119 bytes)
I1202 15:55:16.520348   11180 exec_runner.go:144] found C:\Users\vodka\.minikube/key.pem, removing ...
I1202 15:55:16.520381   11180 exec_runner.go:203] rm: C:\Users\vodka\.minikube\key.pem
I1202 15:55:16.520466   11180 exec_runner.go:151] cp: C:\Users\vodka\.minikube\certs\key.pem --> C:\Users\vodka\.minikube/key.pem (1679 bytes)
I1202 15:55:16.540544   11180 exec_runner.go:144] found C:\Users\vodka\.minikube/ca.pem, removing ...
I1202 15:55:16.540544   11180 exec_runner.go:203] rm: C:\Users\vodka\.minikube\ca.pem
I1202 15:55:16.541131   11180 exec_runner.go:151] cp: C:\Users\vodka\.minikube\certs\ca.pem --> C:\Users\vodka\.minikube/ca.pem (1074 bytes)
I1202 15:55:16.542284   11180 provision.go:117] generating server cert: C:\Users\vodka\.minikube\machines\server.pem ca-key=C:\Users\vodka\.minikube\certs\ca.pem private-key=C:\Users\vodka\.minikube\certs\ca-key.pem org=vodka.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1202 15:55:16.665119   11180 provision.go:177] copyRemoteCerts
I1202 15:55:16.789731   11180 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1202 15:55:16.813986   11180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 15:55:16.930404   11180 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50568 SSHKeyPath:C:\Users\vodka\.minikube\machines\minikube\id_rsa Username:docker}
I1202 15:55:17.091263   11180 ssh_runner.go:362] scp C:\Users\vodka\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1202 15:55:17.147451   11180 ssh_runner.go:362] scp C:\Users\vodka\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I1202 15:55:17.196080   11180 ssh_runner.go:362] scp C:\Users\vodka\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1202 15:55:17.249298   11180 provision.go:87] duration metric: took 914.1342ms to configureAuth
I1202 15:55:17.249298   11180 ubuntu.go:193] setting minikube options for container-runtime
I1202 15:55:17.249902   11180 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I1202 15:55:17.280894   11180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 15:55:17.417090   11180 main.go:141] libmachine: Using SSH client type: native
I1202 15:55:17.423412   11180 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1365360] 0x1367ea0 <nil>  [] 0s} 127.0.0.1 50568 <nil> <nil>}
I1202 15:55:17.423412   11180 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1202 15:55:17.641145   11180 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1202 15:55:17.641145   11180 ubuntu.go:71] root file system type: overlay
I1202 15:55:17.641765   11180 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1202 15:55:17.673288   11180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 15:55:17.802251   11180 main.go:141] libmachine: Using SSH client type: native
I1202 15:55:17.803304   11180 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1365360] 0x1367ea0 <nil>  [] 0s} 127.0.0.1 50568 <nil> <nil>}
I1202 15:55:17.803373   11180 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1202 15:55:18.053794   11180 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1202 15:55:18.101021   11180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 15:55:18.246441   11180 main.go:141] libmachine: Using SSH client type: native
I1202 15:55:18.247019   11180 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1365360] 0x1367ea0 <nil>  [] 0s} 127.0.0.1 50568 <nil> <nil>}
I1202 15:55:18.247019   11180 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1202 15:55:18.462454   11180 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1202 15:55:18.462454   11180 machine.go:96] duration metric: took 6.4335601s to provisionDockerMachine
I1202 15:55:18.463692   11180 start.go:293] postStartSetup for "minikube" (driver="docker")
I1202 15:55:18.463692   11180 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1202 15:55:18.581233   11180 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1202 15:55:18.612113   11180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 15:55:18.711877   11180 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50568 SSHKeyPath:C:\Users\vodka\.minikube\machines\minikube\id_rsa Username:docker}
I1202 15:55:18.961813   11180 ssh_runner.go:195] Run: cat /etc/os-release
I1202 15:55:18.974571   11180 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1202 15:55:18.974684   11180 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1202 15:55:18.974684   11180 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1202 15:55:18.974684   11180 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I1202 15:55:18.975467   11180 filesync.go:126] Scanning C:\Users\vodka\.minikube\addons for local assets ...
I1202 15:55:18.976979   11180 filesync.go:126] Scanning C:\Users\vodka\.minikube\files for local assets ...
I1202 15:55:18.976979   11180 start.go:296] duration metric: took 513.2865ms for postStartSetup
I1202 15:55:19.076211   11180 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1202 15:55:19.093854   11180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 15:55:19.196775   11180 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50568 SSHKeyPath:C:\Users\vodka\.minikube\machines\minikube\id_rsa Username:docker}
I1202 15:55:19.446581   11180 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1202 15:55:19.463407   11180 fix.go:56] duration metric: took 8.8074582s for fixHost
I1202 15:55:19.463407   11180 start.go:83] releasing machines lock for "minikube", held for 8.8080143s
I1202 15:55:19.492950   11180 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1202 15:55:19.614949   11180 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I1202 15:55:19.657263   11180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 15:55:19.735201   11180 ssh_runner.go:195] Run: cat /version.json
I1202 15:55:19.761276   11180 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50568 SSHKeyPath:C:\Users\vodka\.minikube\machines\minikube\id_rsa Username:docker}
I1202 15:55:19.783317   11180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 15:55:19.896919   11180 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50568 SSHKeyPath:C:\Users\vodka\.minikube\machines\minikube\id_rsa Username:docker}
W1202 15:55:19.897471   11180 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I1202 15:55:20.179246   11180 ssh_runner.go:195] Run: systemctl --version
W1202 15:55:20.243253   11180 out.go:270] ‚ùó  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W1202 15:55:20.243253   11180 out.go:270] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1202 15:55:20.355792   11180 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1202 15:55:20.526135   11180 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1202 15:55:20.554978   11180 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1202 15:55:20.702099   11180 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1202 15:55:20.746145   11180 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1202 15:55:20.746145   11180 start.go:495] detecting cgroup driver to use...
I1202 15:55:20.746145   11180 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1202 15:55:20.748982   11180 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1202 15:55:20.982685   11180 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1202 15:55:21.146524   11180 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1202 15:55:21.173681   11180 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1202 15:55:21.288682   11180 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1202 15:55:21.443926   11180 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1202 15:55:21.599100   11180 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1202 15:55:21.755250   11180 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1202 15:55:21.888621   11180 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1202 15:55:22.029302   11180 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1202 15:55:22.188962   11180 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1202 15:55:22.373908   11180 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1202 15:55:22.527217   11180 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1202 15:55:22.726770   11180 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1202 15:55:22.896854   11180 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1202 15:55:23.176714   11180 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1202 15:55:23.378782   11180 start.go:495] detecting cgroup driver to use...
I1202 15:55:23.379366   11180 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1202 15:55:23.560569   11180 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1202 15:55:23.594657   11180 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1202 15:55:23.761429   11180 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1202 15:55:23.802482   11180 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1202 15:55:23.993512   11180 ssh_runner.go:195] Run: which cri-dockerd
I1202 15:55:24.160244   11180 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1202 15:55:24.187590   11180 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I1202 15:55:24.371441   11180 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1202 15:55:24.652822   11180 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1202 15:55:24.815318   11180 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I1202 15:55:24.817171   11180 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1202 15:55:25.029396   11180 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1202 15:55:25.339252   11180 ssh_runner.go:195] Run: sudo systemctl restart docker
I1202 15:55:26.266563   11180 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1202 15:55:26.415654   11180 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1202 15:55:26.561977   11180 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1202 15:55:26.717322   11180 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1202 15:55:26.985487   11180 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1202 15:55:27.339833   11180 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1202 15:55:27.655382   11180 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1202 15:55:27.800156   11180 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1202 15:55:27.956777   11180 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1202 15:55:28.281532   11180 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1202 15:55:29.070151   11180 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1202 15:55:29.215631   11180 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1202 15:55:29.230382   11180 start.go:563] Will wait 60s for crictl version
I1202 15:55:29.323760   11180 ssh_runner.go:195] Run: which crictl
I1202 15:55:29.501973   11180 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1202 15:55:29.787755   11180 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I1202 15:55:29.814956   11180 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1202 15:55:30.067297   11180 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1202 15:55:30.140147   11180 out.go:235] üê≥  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I1202 15:55:30.174892   11180 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1202 15:55:30.459976   11180 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1202 15:55:30.624794   11180 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1202 15:55:30.639980   11180 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1202 15:55:30.705476   11180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1202 15:55:30.816670   11180 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\vodka:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1202 15:55:30.818137   11180 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I1202 15:55:30.838196   11180 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1202 15:55:30.893330   11180 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1202 15:55:30.893330   11180 docker.go:619] Images already preloaded, skipping extraction
I1202 15:55:30.925244   11180 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1202 15:55:30.974413   11180 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1202 15:55:30.976566   11180 cache_images.go:84] Images are preloaded, skipping loading
I1202 15:55:30.976753   11180 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I1202 15:55:30.994088   11180 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1202 15:55:31.015016   11180 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1202 15:55:31.454797   11180 cni.go:84] Creating CNI manager for ""
I1202 15:55:31.454797   11180 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1202 15:55:31.455419   11180 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1202 15:55:31.455419   11180 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1202 15:55:31.456046   11180 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1202 15:55:31.594713   11180 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I1202 15:55:31.626502   11180 binaries.go:44] Found k8s binaries, skipping transfer
I1202 15:55:31.813144   11180 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1202 15:55:31.842639   11180 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1202 15:55:31.893369   11180 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1202 15:55:31.939487   11180 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I1202 15:55:32.114416   11180 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1202 15:55:32.128434   11180 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1202 15:55:32.275977   11180 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1202 15:55:32.568072   11180 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1202 15:55:32.602896   11180 certs.go:68] Setting up C:\Users\vodka\.minikube\profiles\minikube for IP: 192.168.49.2
I1202 15:55:32.602896   11180 certs.go:194] generating shared ca certs ...
I1202 15:55:32.602896   11180 certs.go:226] acquiring lock for ca certs: {Name:mkcb0fcae17ce6978cbc3c0b77cc32c3952c4707 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1202 15:55:32.637394   11180 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\vodka\.minikube\ca.key
I1202 15:55:32.679576   11180 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\vodka\.minikube\proxy-client-ca.key
I1202 15:55:32.679576   11180 certs.go:256] generating profile certs ...
I1202 15:55:32.682058   11180 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\vodka\.minikube\profiles\minikube\client.key
I1202 15:55:32.722055   11180 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\vodka\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I1202 15:55:32.770124   11180 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\vodka\.minikube\profiles\minikube\proxy-client.key
I1202 15:55:32.778129   11180 certs.go:484] found cert: C:\Users\vodka\.minikube\certs\ca-key.pem (1679 bytes)
I1202 15:55:32.778129   11180 certs.go:484] found cert: C:\Users\vodka\.minikube\certs\ca.pem (1074 bytes)
I1202 15:55:32.778933   11180 certs.go:484] found cert: C:\Users\vodka\.minikube\certs\cert.pem (1119 bytes)
I1202 15:55:32.779495   11180 certs.go:484] found cert: C:\Users\vodka\.minikube\certs\key.pem (1679 bytes)
I1202 15:55:32.791459   11180 ssh_runner.go:362] scp C:\Users\vodka\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1202 15:55:32.863825   11180 ssh_runner.go:362] scp C:\Users\vodka\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1202 15:55:32.932270   11180 ssh_runner.go:362] scp C:\Users\vodka\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1202 15:55:32.996522   11180 ssh_runner.go:362] scp C:\Users\vodka\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1202 15:55:33.067704   11180 ssh_runner.go:362] scp C:\Users\vodka\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1202 15:55:33.130332   11180 ssh_runner.go:362] scp C:\Users\vodka\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1202 15:55:33.192561   11180 ssh_runner.go:362] scp C:\Users\vodka\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1202 15:55:33.380283   11180 ssh_runner.go:362] scp C:\Users\vodka\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1202 15:55:33.590520   11180 ssh_runner.go:362] scp C:\Users\vodka\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1202 15:55:33.790951   11180 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1202 15:55:33.922986   11180 ssh_runner.go:195] Run: openssl version
I1202 15:55:34.249006   11180 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1202 15:55:34.492145   11180 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1202 15:55:34.508972   11180 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Dec  1 15:14 /usr/share/ca-certificates/minikubeCA.pem
I1202 15:55:34.520449   11180 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1202 15:55:34.771556   11180 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1202 15:55:34.990223   11180 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1202 15:55:35.021860   11180 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1202 15:55:35.118881   11180 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1202 15:55:35.221784   11180 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1202 15:55:35.315106   11180 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1202 15:55:35.398208   11180 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1202 15:55:35.437798   11180 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1202 15:55:35.512529   11180 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\vodka:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1202 15:55:35.565852   11180 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1202 15:55:35.926839   11180 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1202 15:55:36.014986   11180 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1202 15:55:36.015592   11180 kubeadm.go:593] restartPrimaryControlPlane start ...
I1202 15:55:36.189055   11180 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1202 15:55:36.225414   11180 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1202 15:55:36.262680   11180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1202 15:55:36.428640   11180 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:61765"
I1202 15:55:36.428640   11180 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:61765, want: 127.0.0.1:50572
I1202 15:55:36.431803   11180 kubeconfig.go:62] C:\Users\vodka\.kube\config needs updating (will repair): [kubeconfig needs server address update]
I1202 15:55:36.433408   11180 lock.go:35] WriteFile acquiring C:\Users\vodka\.kube\config: {Name:mkffa6e8337e6a8e36319b048db3c82e23a01fbf Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1202 15:55:36.744151   11180 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1202 15:55:36.811702   11180 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I1202 15:55:36.811702   11180 kubeadm.go:597] duration metric: took 796.1102ms to restartPrimaryControlPlane
I1202 15:55:36.811702   11180 kubeadm.go:394] duration metric: took 1.2991727s to StartCluster
I1202 15:55:36.812264   11180 settings.go:142] acquiring lock: {Name:mk4d94f7dd369e478325d231e79cccac9d1b3a1a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1202 15:55:36.812264   11180 settings.go:150] Updating kubeconfig:  C:\Users\vodka\.kube\config
I1202 15:55:36.815843   11180 lock.go:35] WriteFile acquiring C:\Users\vodka\.kube\config: {Name:mkffa6e8337e6a8e36319b048db3c82e23a01fbf Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1202 15:55:36.818212   11180 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1202 15:55:36.818212   11180 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I1202 15:55:36.819598   11180 out.go:177] üîé  Verifying Kubernetes components...
I1202 15:55:36.820831   11180 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1202 15:55:36.821575   11180 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1202 15:55:36.821575   11180 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1202 15:55:36.822692   11180 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W1202 15:55:36.822692   11180 addons.go:247] addon storage-provisioner should already be in state true
I1202 15:55:36.823240   11180 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1202 15:55:36.823907   11180 host.go:66] Checking if "minikube" exists ...
I1202 15:55:36.949616   11180 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1202 15:55:36.951383   11180 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1202 15:55:37.076864   11180 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1202 15:55:37.092872   11180 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1202 15:55:37.097988   11180 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1202 15:55:37.097988   11180 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1202 15:55:37.112166   11180 addons.go:238] Setting addon default-storageclass=true in "minikube"
W1202 15:55:37.112166   11180 addons.go:247] addon default-storageclass should already be in state true
I1202 15:55:37.112761   11180 host.go:66] Checking if "minikube" exists ...
I1202 15:55:37.148202   11180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 15:55:37.231328   11180 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1202 15:55:37.310058   11180 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50568 SSHKeyPath:C:\Users\vodka\.minikube\machines\minikube\id_rsa Username:docker}
I1202 15:55:37.367829   11180 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I1202 15:55:37.367829   11180 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1202 15:55:37.425575   11180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1202 15:55:37.551054   11180 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50568 SSHKeyPath:C:\Users\vodka\.minikube\machines\minikube\id_rsa Username:docker}
I1202 15:55:38.115111   11180 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (1.0382464s)
I1202 15:55:38.324423   11180 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1202 15:55:38.338746   11180 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1202 15:55:38.348415   11180 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1202 15:55:39.400744   11180 ssh_runner.go:235] Completed: sudo systemctl start kubelet: (1.0518126s)
I1202 15:55:39.400744   11180 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.0614815s)
I1202 15:55:39.400797   11180 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.0763741s)
W1202 15:55:39.400797   11180 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1202 15:55:39.400797   11180 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1202 15:55:39.400797   11180 retry.go:31] will retry after 238.627467ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1202 15:55:39.400797   11180 retry.go:31] will retry after 374.560509ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1202 15:55:39.438898   11180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1202 15:55:39.557688   11180 api_server.go:52] waiting for apiserver process to appear ...
I1202 15:55:39.731122   11180 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1202 15:55:39.800167   11180 api_server.go:72] duration metric: took 2.9819545s to wait for apiserver process to appear ...
I1202 15:55:39.800167   11180 api_server.go:88] waiting for apiserver healthz status ...
I1202 15:55:39.800167   11180 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50572/healthz ...
I1202 15:55:39.820652   11180 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1202 15:55:39.951225   11180 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1202 15:55:44.803315   11180 api_server.go:269] stopped: https://127.0.0.1:50572/healthz: Get "https://127.0.0.1:50572/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1202 15:55:44.803315   11180 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50572/healthz ...
I1202 15:55:45.104675   11180 api_server.go:279] https://127.0.0.1:50572/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1202 15:55:45.104675   11180 api_server.go:103] status: https://127.0.0.1:50572/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1202 15:55:45.300453   11180 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50572/healthz ...
I1202 15:55:45.487489   11180 api_server.go:279] https://127.0.0.1:50572/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1202 15:55:45.487489   11180 api_server.go:103] status: https://127.0.0.1:50572/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1202 15:55:45.800645   11180 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50572/healthz ...
I1202 15:55:45.873011   11180 api_server.go:279] https://127.0.0.1:50572/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1202 15:55:45.873011   11180 api_server.go:103] status: https://127.0.0.1:50572/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1202 15:55:46.103673   11180 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (6.2830212s)
I1202 15:55:46.300669   11180 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50572/healthz ...
I1202 15:55:46.390936   11180 api_server.go:279] https://127.0.0.1:50572/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1202 15:55:46.390936   11180 api_server.go:103] status: https://127.0.0.1:50572/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1202 15:55:46.800500   11180 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50572/healthz ...
I1202 15:55:46.880934   11180 api_server.go:279] https://127.0.0.1:50572/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1202 15:55:46.880955   11180 api_server.go:103] status: https://127.0.0.1:50572/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1202 15:55:47.300879   11180 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50572/healthz ...
I1202 15:55:47.392262   11180 api_server.go:279] https://127.0.0.1:50572/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1202 15:55:47.392262   11180 api_server.go:103] status: https://127.0.0.1:50572/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1202 15:55:47.800398   11180 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50572/healthz ...
I1202 15:55:47.881041   11180 api_server.go:279] https://127.0.0.1:50572/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1202 15:55:47.881041   11180 api_server.go:103] status: https://127.0.0.1:50572/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1202 15:55:48.300678   11180 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50572/healthz ...
I1202 15:55:48.385118   11180 api_server.go:279] https://127.0.0.1:50572/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1202 15:55:48.385118   11180 api_server.go:103] status: https://127.0.0.1:50572/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1202 15:55:48.800627   11180 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50572/healthz ...
I1202 15:55:48.889434   11180 api_server.go:279] https://127.0.0.1:50572/healthz returned 200:
ok
I1202 15:55:48.897193   11180 api_server.go:141] control plane version: v1.32.0
I1202 15:55:48.897193   11180 api_server.go:131] duration metric: took 9.097026s to wait for apiserver health ...
I1202 15:55:48.898483   11180 system_pods.go:43] waiting for kube-system pods to appear ...
I1202 15:55:49.011608   11180 system_pods.go:59] 7 kube-system pods found
I1202 15:55:49.011608   11180 system_pods.go:61] "coredns-668d6bf9bc-6bwpc" [d6ec834a-dc5b-48af-b11a-da921c254499] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1202 15:55:49.011608   11180 system_pods.go:61] "etcd-minikube" [f527b5c2-4e82-41aa-915f-091ed1c58e10] Running
I1202 15:55:49.011608   11180 system_pods.go:61] "kube-apiserver-minikube" [e60ce4b7-d2e3-42a8-b511-3796fbc3d3b0] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1202 15:55:49.011608   11180 system_pods.go:61] "kube-controller-manager-minikube" [333ffec6-1f6d-4982-88f8-7f209a7d73e3] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1202 15:55:49.011608   11180 system_pods.go:61] "kube-proxy-sxdcr" [80e91b62-7930-4c1a-95b1-008c06d23b13] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1202 15:55:49.011608   11180 system_pods.go:61] "kube-scheduler-minikube" [6583724e-7e0b-4586-acb2-09c2f2eeffda] Running
I1202 15:55:49.011608   11180 system_pods.go:61] "storage-provisioner" [e78294b9-5fc3-42af-93df-7ef1194fa5e1] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1202 15:55:49.011608   11180 system_pods.go:74] duration metric: took 113.1251ms to wait for pod list to return data ...
I1202 15:55:49.011608   11180 kubeadm.go:582] duration metric: took 12.1933956s to wait for: map[apiserver:true system_pods:true]
I1202 15:55:49.011608   11180 node_conditions.go:102] verifying NodePressure condition ...
I1202 15:55:49.082108   11180 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1202 15:55:49.082108   11180 node_conditions.go:123] node cpu capacity is 8
I1202 15:55:49.082108   11180 node_conditions.go:105] duration metric: took 70.4996ms to run NodePressure ...
I1202 15:55:49.082108   11180 start.go:241] waiting for startup goroutines ...
I1202 15:55:50.701549   11180 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (10.7503238s)
I1202 15:55:50.705163   11180 out.go:177] üåü  Enabled addons: default-storageclass, storage-provisioner
I1202 15:55:50.706749   11180 addons.go:514] duration metric: took 13.8891072s for enable addons: enabled=[default-storageclass storage-provisioner]
I1202 15:55:50.706749   11180 start.go:246] waiting for cluster config update ...
I1202 15:55:50.706749   11180 start.go:255] writing updated cluster config ...
I1202 15:55:50.887207   11180 ssh_runner.go:195] Run: rm -f paused
I1202 15:55:51.167834   11180 start.go:600] kubectl: 1.29.2, cluster: 1.32.0 (minor skew: 3)
I1202 15:55:51.169059   11180 out.go:201] 
W1202 15:55:51.170772   11180 out.go:270] ‚ùó  C:\Program Files\Docker\Docker\resources\bin\kubectl.exe is version 1.29.2, which may have incompatibilities with Kubernetes 1.32.0.
I1202 15:55:51.173787   11180 out.go:177]     ‚ñ™ Want kubectl v1.32.0? Try 'minikube kubectl -- get pods -A'
I1202 15:55:51.178628   11180 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Dec 02 19:47:01 minikube dockerd[1126]: time="2025-12-02T19:47:01.784021607Z" level=info msg="ignoring event" container=27c48e0af0d5a0f6db8ead6d9cd324efe7e16aa18b3b6124d95ff153f5b903e7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 19:50:05 minikube dockerd[1126]: time="2025-12-02T19:50:05.173383584Z" level=info msg="ignoring event" container=2c188cd1b8d865b3fda0c40eb219f2b1122a86c8a346e51d67f249482f117a45 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 19:50:08 minikube dockerd[1126]: time="2025-12-02T19:50:08.677530152Z" level=info msg="ignoring event" container=5a222fd60b0e30b6004e12e8ed9e48b1cec3e826d395fc203654faa459cf7cd1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 19:50:33 minikube dockerd[1126]: time="2025-12-02T19:50:33.213376012Z" level=info msg="ignoring event" container=7e83d3bd0cfb98b40c815dc3969df5117c05a85bcfe5fdf05858b542697486c6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 19:53:01 minikube dockerd[1126]: time="2025-12-02T19:53:01.179368688Z" level=info msg="ignoring event" container=64fa293044a13c93622599a6430278b73511908a5471f5936e8b9d28f6035b53 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 19:53:20 minikube dockerd[1126]: time="2025-12-02T19:53:20.799222413Z" level=info msg="ignoring event" container=c4329eda8f3903c2ada2d46536b7adcce938d2b0139da867a111dd90b61004df module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 19:55:33 minikube dockerd[1126]: time="2025-12-02T19:55:33.992124960Z" level=info msg="ignoring event" container=0700d2d68a02e4102e7e1cd2a303036015245c3f02bc5728611aa9b0ac087029 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 19:55:54 minikube dockerd[1126]: time="2025-12-02T19:55:54.871579626Z" level=info msg="ignoring event" container=a3700a1f7bc8b755f6d82afdb0a47c7d6ff9ad7c0a9e1036b8efbd23385ea9b4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 19:58:16 minikube dockerd[1126]: time="2025-12-02T19:58:16.059683512Z" level=info msg="ignoring event" container=edd2ecda4a19ba04ddce4c680a5548fb91150d6359e83d185ac5beb9df184f7e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 19:58:42 minikube dockerd[1126]: time="2025-12-02T19:58:42.080691498Z" level=info msg="ignoring event" container=6b49cd960b0a2f2140983290a6b996cc34f8719907d0c1541b0377611531a734 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:01:41 minikube dockerd[1126]: time="2025-12-02T20:01:41.933075114Z" level=info msg="ignoring event" container=72424ee345feb9b2e1aa4a67f6dcfc70ed8438187ae459dd6a2efdae5efd590b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:05:14 minikube dockerd[1126]: time="2025-12-02T20:05:14.833760357Z" level=info msg="ignoring event" container=e7c7ee2850b0865ffb9300bf7c5f3c387c2fad3cedfc5fe3b7f38eb41fd7d84c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:09:53 minikube dockerd[1126]: time="2025-12-02T20:09:53.522793777Z" level=info msg="ignoring event" container=fd415290ffe497661c0bc691768867b46f15ed2b5c104629c09ac463c26c75ba module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:14:49 minikube dockerd[1126]: time="2025-12-02T20:14:49.347468987Z" level=info msg="ignoring event" container=4e6a22919cc8a903caeaeb972f3db84267185b25d37d8cb9b1a7bfbfecd33577 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:14:49 minikube dockerd[1126]: time="2025-12-02T20:14:49.666979811Z" level=info msg="ignoring event" container=c0521ed91325f3af94e633ab30e44d660c9ec895c81ab555e8b8541893b2102e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:14:49 minikube dockerd[1126]: time="2025-12-02T20:14:49.975383456Z" level=info msg="ignoring event" container=b040310a0b3ccba8b5a8a38a84f2c0316b8fd6c101eb921828c9eec6e5235e7c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:18:46 minikube cri-dockerd[1434]: E1202 20:18:46.423131    1434 httpstream.go:257] error forwarding port 9632 to pod 346ce64fa77ea68faf9f7ff323c8ea091da7eef41dd26178bc8253ed9c582b79, uid : Error response from daemon: No such container: 346ce64fa77ea68faf9f7ff323c8ea091da7eef41dd26178bc8253ed9c582b79
Dec 02 20:18:46 minikube cri-dockerd[1434]: E1202 20:18:46.423092    1434 httpstream.go:257] error forwarding port 9632 to pod 346ce64fa77ea68faf9f7ff323c8ea091da7eef41dd26178bc8253ed9c582b79, uid : Error response from daemon: No such container: 346ce64fa77ea68faf9f7ff323c8ea091da7eef41dd26178bc8253ed9c582b79
Dec 02 20:19:50 minikube dockerd[1126]: time="2025-12-02T20:19:50.236387040Z" level=info msg="ignoring event" container=7da7c20ccee64ac7cfd7d50cd73b1afa5e6e04ab5c77b5082c7b990bc99f2c52 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:19:50 minikube dockerd[1126]: time="2025-12-02T20:19:50.625328565Z" level=info msg="ignoring event" container=41b21115864270cc74def19a6eab4b53bec706310fc241c803c66da4f804e99c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:43:12 minikube cri-dockerd[1434]: time="2025-12-02T20:43:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cc6a7e2cd89e909722637dafded8e9ab2816183d178fcd869a606543fc720241/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 02 20:43:15 minikube dockerd[1126]: time="2025-12-02T20:43:15.327230785Z" level=info msg="ignoring event" container=9c155ea6fa04381150b76bc838fefbc5edec0330a915728ec33e489863158ae9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:43:16 minikube dockerd[1126]: time="2025-12-02T20:43:16.093230878Z" level=info msg="ignoring event" container=f5e9dbfefa81af2e5899180b80227833ae41bfdf156e71b609282b0af0f6ee12 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:44:26 minikube cri-dockerd[1434]: time="2025-12-02T20:44:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5f0687d695f19780ebc74c1f68559b815285dc82350fb3e66d55a8fab641d21f/resolv.conf as [nameserver 10.96.0.10 search dev.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 02 20:44:28 minikube dockerd[1126]: time="2025-12-02T20:44:28.746148497Z" level=info msg="ignoring event" container=6a2ba1936cb3687d265c8ecd6cab6bcd9e79951ebd6c8a1a3e2174316e585680 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:44:29 minikube cri-dockerd[1434]: time="2025-12-02T20:44:29Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dev-backend-manga2you-v1-676f47fbc9-8v897_dev\": unexpected command output nsenter: cannot open /proc/57001/ns/net: No such file or directory\n with error: exit status 1"
Dec 02 20:44:29 minikube dockerd[1126]: time="2025-12-02T20:44:29.043042335Z" level=info msg="ignoring event" container=d98341cbf8653827fef69691cdcc4001a1fd942b5b974b90f0cb0e03749287a0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:44:51 minikube cri-dockerd[1434]: time="2025-12-02T20:44:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9bf0cbed55134086a12a4c1ee0bb12c7529d760c52f345e10ea8251dd73c437a/resolv.conf as [nameserver 10.96.0.10 search dev.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 02 20:44:51 minikube cri-dockerd[1434]: time="2025-12-02T20:44:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dbb9b0e2eaa7b54706082f2cc07fb3bb93e184e974f293c2a4690acbea32b768/resolv.conf as [nameserver 10.96.0.10 search dev.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 02 20:44:51 minikube cri-dockerd[1434]: time="2025-12-02T20:44:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/93fc53bec17aa342342e3d2d2176282ca073be80195a843b98af766cba787901/resolv.conf as [nameserver 10.96.0.10 search dev.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 02 20:46:07 minikube dockerd[1126]: time="2025-12-02T20:46:07.795727307Z" level=info msg="ignoring event" container=dc198792728a033618574577a9f196e8a1db42bf24f7ff932a4ebd9ba15a14b9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:47:20 minikube dockerd[1126]: time="2025-12-02T20:47:20.072860882Z" level=info msg="ignoring event" container=656adcd6cb8906762a0eccb2475844aa36fce0ef645799a51db909e661c147f9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:48:47 minikube dockerd[1126]: time="2025-12-02T20:48:47.586381842Z" level=info msg="ignoring event" container=514afe2eadb20aec11fb7c0b1cc93081b12d80dbec7e33fcc46e55607d251782 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:49:15 minikube dockerd[1126]: time="2025-12-02T20:49:15.488341194Z" level=info msg="ignoring event" container=ff432710200cef99b3a3618f16416458745ab7da9d81dc68c29fb21e270015e2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:50:25 minikube dockerd[1126]: time="2025-12-02T20:50:25.479658931Z" level=info msg="ignoring event" container=47221d91fda77ab8780d7b33a5e3e5649cfcf8863ef0582dbc2ec8f281b848c3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:52:08 minikube dockerd[1126]: time="2025-12-02T20:52:08.766386393Z" level=info msg="ignoring event" container=14723b329d8c4d3bb7f2f05b2bc8c292a92825aa7979adc5a6a64602aa55ebd4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:53:51 minikube dockerd[1126]: time="2025-12-02T20:53:51.645163697Z" level=info msg="ignoring event" container=0892051d5b1a26cc60879b7c6e473865e802ab6e0a2a5ad120a53901e34e0f66 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:56:05 minikube dockerd[1126]: time="2025-12-02T20:56:05.519948272Z" level=info msg="ignoring event" container=3ef9348f4c2e381fe1dc9cd4d772e2f195b444cd25621962e3849880fc53bc31 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 20:59:56 minikube dockerd[1126]: time="2025-12-02T20:59:56.624018782Z" level=info msg="ignoring event" container=0dd9dfd08a75084fe59a0fdd7eeb02486e0169fb70baebe46f682d44cb4c63eb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 21:04:55 minikube dockerd[1126]: time="2025-12-02T21:04:55.864188371Z" level=info msg="ignoring event" container=5f0687d695f19780ebc74c1f68559b815285dc82350fb3e66d55a8fab641d21f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 21:04:56 minikube dockerd[1126]: time="2025-12-02T21:04:56.102508097Z" level=info msg="ignoring event" container=40e52a42606d2f19a40a93d0c6eac76f17f4fbb471ebb43408e4238b7f47b842 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 21:04:56 minikube cri-dockerd[1434]: time="2025-12-02T21:04:56Z" level=error msg="error getting RW layer size for container ID '0dd9dfd08a75084fe59a0fdd7eeb02486e0169fb70baebe46f682d44cb4c63eb': Error response from daemon: No such container: 0dd9dfd08a75084fe59a0fdd7eeb02486e0169fb70baebe46f682d44cb4c63eb"
Dec 02 21:04:56 minikube cri-dockerd[1434]: time="2025-12-02T21:04:56Z" level=error msg="Set backoffDuration to : 1m0s for container ID '0dd9dfd08a75084fe59a0fdd7eeb02486e0169fb70baebe46f682d44cb4c63eb'"
Dec 02 21:04:56 minikube dockerd[1126]: time="2025-12-02T21:04:56.493108595Z" level=info msg="ignoring event" container=9bf0cbed55134086a12a4c1ee0bb12c7529d760c52f345e10ea8251dd73c437a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 21:09:56 minikube dockerd[1126]: time="2025-12-02T21:09:56.062110739Z" level=info msg="ignoring event" container=55c17b624428b3b7af7e2bc201446f943042e58391e62e6f13d625930445a188 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 21:09:56 minikube dockerd[1126]: time="2025-12-02T21:09:56.272046720Z" level=info msg="ignoring event" container=93fc53bec17aa342342e3d2d2176282ca073be80195a843b98af766cba787901 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 02 21:22:10 minikube dockerd[1126]: time="2025-12-02T21:22:10.407613598Z" level=warning msg="Published ports are discarded when using host network mode"
Dec 02 21:22:10 minikube dockerd[1126]: time="2025-12-02T21:22:10.461716163Z" level=warning msg="Published ports are discarded when using host network mode"
Dec 02 21:22:10 minikube cri-dockerd[1434]: time="2025-12-02T21:22:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/48ae625552e58673e019696823e0eb0e5b8e42a8b4d027de11b0470e20351c24/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 02 21:22:11 minikube dockerd[1126]: time="2025-12-02T21:22:11.668109568Z" level=info msg="Attempting next endpoint for pull after error: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials"
Dec 02 21:22:11 minikube dockerd[1126]: time="2025-12-02T21:22:11.672670559Z" level=error msg="Handler for POST /v1.43/images/create returned error: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials"
Dec 02 21:22:28 minikube dockerd[1126]: time="2025-12-02T21:22:28.070864936Z" level=info msg="Attempting next endpoint for pull after error: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials"
Dec 02 21:22:28 minikube dockerd[1126]: time="2025-12-02T21:22:28.075267144Z" level=error msg="Handler for POST /v1.43/images/create returned error: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials"
Dec 02 21:22:54 minikube dockerd[1126]: time="2025-12-02T21:22:54.100869840Z" level=info msg="Attempting next endpoint for pull after error: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials"
Dec 02 21:22:54 minikube dockerd[1126]: time="2025-12-02T21:22:54.107940674Z" level=error msg="Handler for POST /v1.43/images/create returned error: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials"
Dec 02 21:23:37 minikube dockerd[1126]: time="2025-12-02T21:23:37.153515502Z" level=info msg="Attempting next endpoint for pull after error: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials"
Dec 02 21:23:37 minikube dockerd[1126]: time="2025-12-02T21:23:37.167390049Z" level=error msg="Handler for POST /v1.43/images/create returned error: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials"
Dec 02 21:25:06 minikube dockerd[1126]: time="2025-12-02T21:25:06.147981274Z" level=info msg="Attempting next endpoint for pull after error: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials"
Dec 02 21:25:06 minikube dockerd[1126]: time="2025-12-02T21:25:06.152632189Z" level=error msg="Handler for POST /v1.43/images/create returned error: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials"
Dec 02 21:25:06 minikube dockerd[1126]: time="2025-12-02T21:25:06.964104873Z" level=info msg="ignoring event" container=48ae625552e58673e019696823e0eb0e5b8e42a8b4d027de11b0470e20351c24 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE                                                                           CREATED             STATE               NAME                          ATTEMPT             POD ID              POD
ba0526767165d       46076c800f4fc                                                                   40 minutes ago      Running             backend-manga2you-container   0                   dbb9b0e2eaa7b       dev-backend-manga2you-v1-867f497d67-7mlmh
b232e41825d3a       46076c800f4fc                                                                   42 minutes ago      Running             backend-manga2you-container   0                   cc6a7e2cd89e9       backend-manga2you-575c99d7fb-nbr4p
6930e902b03ee       1c655933b9c56                                                                   2 hours ago         Running             metrics-server                0                   9e1fc83afb630       metrics-server-86447695c5-4nhc6
857a60e24ae4f       nginx@sha256:b3c656d55d7ad751196f21b7fd2e8d4da9cb430e32f646adcf92441b72f82b14   6 hours ago         Running             nginx-container               0                   a6467e96357f6       dev-nginx-deployment-v1-57fdbd695f-kcbvq
d18907b60bc47       6e38f40d628db                                                                   6 hours ago         Running             storage-provisioner           5                   a9d92e5cafa14       storage-provisioner
ad4254c3084cb       c69fa2e9cbf5f                                                                   6 hours ago         Running             coredns                       2                   7e3b83c9dc2d7       coredns-668d6bf9bc-6bwpc
3b1b5163e4eaa       040f9f8aac8cd                                                                   6 hours ago         Running             kube-proxy                    2                   943ee258fd699       kube-proxy-sxdcr
9007c0048dc1a       6e38f40d628db                                                                   6 hours ago         Exited              storage-provisioner           4                   a9d92e5cafa14       storage-provisioner
ea4c0cf47fcc8       c2e17b8d0f4a3                                                                   7 hours ago         Running             kube-apiserver                2                   547cfbe78ff3c       kube-apiserver-minikube
63111d623a0d9       8cab3d2a8bd0f                                                                   7 hours ago         Running             kube-controller-manager       2                   7b48922a5ce48       kube-controller-manager-minikube
6e0ad7990fd29       a389e107f4ff1                                                                   7 hours ago         Running             kube-scheduler                2                   2c354f87f78ba       kube-scheduler-minikube
f7c30c4f28780       a9e7e6b294baf                                                                   7 hours ago         Running             etcd                          2                   fb151df81625d       etcd-minikube
af9461e4bfa32       c69fa2e9cbf5f                                                                   30 hours ago        Exited              coredns                       1                   762d50db949a2       coredns-668d6bf9bc-6bwpc
d8b66790a4c1a       040f9f8aac8cd                                                                   30 hours ago        Exited              kube-proxy                    1                   768d1fe1a877b       kube-proxy-sxdcr
ddc2c36144b9f       a389e107f4ff1                                                                   30 hours ago        Exited              kube-scheduler                1                   06c0fca5f4ba5       kube-scheduler-minikube
e7467ac06a70b       8cab3d2a8bd0f                                                                   30 hours ago        Exited              kube-controller-manager       1                   05f786d962966       kube-controller-manager-minikube
a480a505fe284       a9e7e6b294baf                                                                   30 hours ago        Exited              etcd                          1                   cdb162d6db7c7       etcd-minikube
66eb19b9895fb       c2e17b8d0f4a3                                                                   30 hours ago        Exited              kube-apiserver                1                   6869d589528bb       kube-apiserver-minikube


==> coredns [ad4254c3084c] <==
[INFO] 10.244.0.41:38393 - 11177 "AAAA IN host.minikube.internal.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000131406s
[INFO] 10.244.0.41:38393 - 10977 "A IN host.minikube.internal.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000172309s
[INFO] 10.244.0.41:33367 - 25124 "AAAA IN host.minikube.internal. udp 40 false 512" NOERROR qr,aa,rd 40 0.000181409s
[INFO] 10.244.0.41:33367 - 24824 "A IN host.minikube.internal. udp 40 false 512" NOERROR qr,aa,rd 78 0.000289614s
[INFO] 10.244.0.41:41346 - 10598 "AAAA IN host.minikube.internal.default.svc.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.000255012s
[INFO] 10.244.0.41:41346 - 10298 "A IN host.minikube.internal.default.svc.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.000375018s
[INFO] 10.244.0.41:59453 - 12449 "AAAA IN host.minikube.internal.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000184609s
[INFO] 10.244.0.41:59453 - 12149 "A IN host.minikube.internal.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000326616s
[INFO] 10.244.0.41:49976 - 46964 "AAAA IN host.minikube.internal.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000241011s
[INFO] 10.244.0.41:49976 - 46564 "A IN host.minikube.internal.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000242412s
[INFO] 10.244.0.41:33843 - 30545 "AAAA IN host.minikube.internal. udp 40 false 512" NOERROR qr,aa,rd 40 0.000229511s
[INFO] 10.244.0.41:33843 - 30145 "A IN host.minikube.internal. udp 40 false 512" NOERROR qr,aa,rd 78 0.000381419s
[INFO] 10.244.0.41:58908 - 37543 "A IN host.minikube.internal.default.svc.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.000394619s
[INFO] 10.244.0.41:58908 - 37943 "AAAA IN host.minikube.internal.default.svc.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.000525826s
[INFO] 10.244.0.41:43370 - 11500 "AAAA IN host.minikube.internal.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000286114s
[INFO] 10.244.0.41:43370 - 11100 "A IN host.minikube.internal.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.00043212s
[INFO] 10.244.0.41:46507 - 43771 "A IN host.minikube.internal.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000304415s
[INFO] 10.244.0.41:46507 - 44571 "AAAA IN host.minikube.internal.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000524825s
[INFO] 10.244.0.41:50787 - 7299 "AAAA IN host.minikube.internal. udp 40 false 512" NOERROR qr,aa,rd 40 0.000345216s
[INFO] 10.244.0.41:50787 - 6899 "A IN host.minikube.internal. udp 40 false 512" NOERROR qr,aa,rd 78 0.000535526s
[INFO] 10.244.0.44:41292 - 2762 "AAAA IN host.minikube.internal.dev.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000258312s
[INFO] 10.244.0.44:41292 - 2362 "A IN host.minikube.internal.dev.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000301915s
[INFO] 10.244.0.44:37436 - 54284 "AAAA IN host.minikube.internal.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000303915s
[INFO] 10.244.0.44:37436 - 53784 "A IN host.minikube.internal.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000459922s
[INFO] 10.244.0.44:59256 - 62304 "AAAA IN host.minikube.internal.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000271713s
[INFO] 10.244.0.44:59256 - 62004 "A IN host.minikube.internal.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000509125s
[INFO] 10.244.0.44:33499 - 24889 "AAAA IN host.minikube.internal. udp 40 false 512" NOERROR qr,aa,rd 40 0.000322616s
[INFO] 10.244.0.44:33499 - 24489 "A IN host.minikube.internal. udp 40 false 512" NOERROR qr,aa,rd 78 0.000224211s
[INFO] 10.244.0.44:54004 - 39081 "AAAA IN host.minikube.internal.dev.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000296214s
[INFO] 10.244.0.44:54004 - 38881 "A IN host.minikube.internal.dev.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000409618s
[INFO] 10.244.0.44:54388 - 24528 "AAAA IN host.minikube.internal.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000163907s
[INFO] 10.244.0.44:54388 - 24228 "A IN host.minikube.internal.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000145706s
[INFO] 10.244.0.44:38719 - 25972 "AAAA IN host.minikube.internal.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000163308s
[INFO] 10.244.0.44:38719 - 25672 "A IN host.minikube.internal.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000139406s
[INFO] 10.244.0.44:59731 - 1114 "AAAA IN host.minikube.internal. udp 40 false 512" NOERROR qr,aa,rd 40 0.000192609s
[INFO] 10.244.0.44:59731 - 814 "A IN host.minikube.internal. udp 40 false 512" NOERROR qr,aa,rd 78 0.000282013s
[INFO] 10.244.0.44:48942 - 56185 "AAAA IN host.minikube.internal.dev.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000317715s
[INFO] 10.244.0.44:48942 - 55985 "A IN host.minikube.internal.dev.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000465322s
[INFO] 10.244.0.44:56994 - 43070 "AAAA IN host.minikube.internal.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000236412s
[INFO] 10.244.0.44:56994 - 42670 "A IN host.minikube.internal.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000292914s
[INFO] 10.244.0.44:43337 - 53420 "AAAA IN host.minikube.internal.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000136907s
[INFO] 10.244.0.44:43337 - 53220 "A IN host.minikube.internal.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.00020801s
[INFO] 10.244.0.44:39015 - 49729 "AAAA IN host.minikube.internal. udp 40 false 512" NOERROR qr,aa,rd 40 0.000133207s
[INFO] 10.244.0.44:39015 - 49629 "A IN host.minikube.internal. udp 40 false 512" NOERROR qr,aa,rd 78 0.000172809s
[INFO] 10.244.0.41:53580 - 43029 "AAAA IN host.minikube.internal.default.svc.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.000230411s
[INFO] 10.244.0.41:53580 - 42629 "A IN host.minikube.internal.default.svc.cluster.local. udp 66 false 512" NXDOMAIN qr,aa,rd 159 0.000175108s
[INFO] 10.244.0.41:53856 - 47514 "AAAA IN host.minikube.internal.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000235811s
[INFO] 10.244.0.41:53856 - 47114 "A IN host.minikube.internal.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000352316s
[INFO] 10.244.0.41:33540 - 24929 "A IN host.minikube.internal.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000257312s
[INFO] 10.244.0.41:33540 - 25329 "AAAA IN host.minikube.internal.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000171508s
[INFO] 10.244.0.41:40731 - 51904 "AAAA IN host.minikube.internal. udp 40 false 512" NOERROR qr,aa,rd 40 0.000289313s
[INFO] 10.244.0.41:40731 - 51704 "A IN host.minikube.internal. udp 40 false 512" NOERROR qr,aa,rd 78 0.000385518s
[INFO] 10.244.0.44:45012 - 61987 "AAAA IN host.minikube.internal.dev.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000243311s
[INFO] 10.244.0.44:45012 - 61787 "A IN host.minikube.internal.dev.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000261912s
[INFO] 10.244.0.44:57668 - 27434 "AAAA IN host.minikube.internal.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000144306s
[INFO] 10.244.0.44:57668 - 27134 "A IN host.minikube.internal.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.00020871s
[INFO] 10.244.0.44:42988 - 31643 "AAAA IN host.minikube.internal.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000157607s
[INFO] 10.244.0.44:42988 - 31343 "A IN host.minikube.internal.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.00021161s
[INFO] 10.244.0.44:59986 - 6873 "AAAA IN host.minikube.internal. udp 40 false 512" NOERROR qr,aa,rd 40 0.000065303s
[INFO] 10.244.0.44:59986 - 6417 "A IN host.minikube.internal. udp 40 false 512" NOERROR qr,aa,rd 78 0.000080703s


==> coredns [af9461e4bfa3] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:35343 - 65233 "HINFO IN 6322259951599876736.565760531899359495. udp 56 false 512" - - 0 6.003077611s
[ERROR] plugin/errors: 2 6322259951599876736.565760531899359495. HINFO: read udp 10.244.0.3:38818->192.168.65.254:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:48996 - 61202 "HINFO IN 6322259951599876736.565760531899359495. udp 56 false 512" - - 0 6.001639943s
[ERROR] plugin/errors: 2 6322259951599876736.565760531899359495. HINFO: read udp 10.244.0.3:52216->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:56307 - 50513 "HINFO IN 6322259951599876736.565760531899359495. udp 56 false 512" - - 0 4.001436126s
[ERROR] plugin/errors: 2 6322259951599876736.565760531899359495. HINFO: read udp 10.244.0.3:59663->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:32909 - 52570 "HINFO IN 6322259951599876736.565760531899359495. udp 56 false 512" - - 0 2.000538713s
[ERROR] plugin/errors: 2 6322259951599876736.565760531899359495. HINFO: read udp 10.244.0.3:59335->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:51710 - 9685 "HINFO IN 6322259951599876736.565760531899359495. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.002940546s
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1840082738]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (01-Dec-2025 15:20:52.553) (total time: 21008ms):
Trace[1840082738]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21008ms (15:21:13.561)
Trace[1840082738]: [21.008227756s] [21.008227756s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1465126974]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (01-Dec-2025 15:20:52.552) (total time: 21024ms):
Trace[1465126974]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21024ms (15:21:13.577)
Trace[1465126974]: [21.024667089s] [21.024667089s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[840966787]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (01-Dec-2025 15:20:52.552) (total time: 21024ms):
Trace[840966787]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21024ms (15:21:13.577)
Trace[840966787]: [21.024674489s] [21.024674489s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/ready: Still waiting on: "kubernetes"


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_12_01T16_14_24_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 01 Dec 2025 15:14:21 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 02 Dec 2025 21:25:43 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 02 Dec 2025 21:22:58 +0000   Mon, 01 Dec 2025 15:14:19 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 02 Dec 2025 21:22:58 +0000   Mon, 01 Dec 2025 15:14:19 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 02 Dec 2025 21:22:58 +0000   Mon, 01 Dec 2025 15:14:19 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 02 Dec 2025 21:22:58 +0000   Tue, 02 Dec 2025 19:47:08 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8079392Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8079392Ki
  pods:               110
System Info:
  Machine ID:                 b6557e700e8a4e1896bd86c184b6b4af
  System UUID:                b6557e700e8a4e1896bd86c184b6b4af
  Boot ID:                    bcfd7f8e-af74-4a5a-8e31-cc3e23f2d6df
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     backend-manga2you-575c99d7fb-nbr4p           0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m
  default                     dev-nginx-deployment-v1-57fdbd695f-kcbvq     0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h26m
  dev                         dev-backend-manga2you-v1-867f497d67-7mlmh    200m (2%)     500m (6%)   128Mi (1%)       256Mi (3%)     40m
  kube-system                 coredns-668d6bf9bc-6bwpc                     100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     30h
  kube-system                 etcd-minikube                                100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         30h
  kube-system                 kube-apiserver-minikube                      250m (3%)     0 (0%)      0 (0%)           0 (0%)         30h
  kube-system                 kube-controller-manager-minikube             200m (2%)     0 (0%)      0 (0%)           0 (0%)         30h
  kube-system                 kube-proxy-sxdcr                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         30h
  kube-system                 kube-scheduler-minikube                      100m (1%)     0 (0%)      0 (0%)           0 (0%)         30h
  kube-system                 metrics-server-86447695c5-4nhc6              100m (1%)     0 (0%)      200Mi (2%)       0 (0%)         131m
  kube-system                 storage-provisioner                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         30h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1050m (13%)  500m (6%)
  memory             498Mi (6%)   426Mi (5%)
  ephemeral-storage  0 (0%)       0 (0%)
  hugepages-1Gi      0 (0%)       0 (0%)
  hugepages-2Mi      0 (0%)       0 (0%)
Events:              <none>


==> dmesg <==
[  +0.000001] RBP: 00007fa3942d8ec0 R08: 00007fa345291b80 R09: 0000000000000000
[  +0.000001] R10: 00007fa345352b60 R11: 0000000000000246 R12: 0000000000080000
[  +0.000001] R13: 00007fa3942db910 R14: 0000000000040000 R15: 0000000000000000
[  +0.000002]  </TASK>
[  +0.000166] Memory cgroup out of memory: Killed process 76641 (java) total-vm:1829868kB, anon-rss:256396kB, file-rss:19952kB, shmem-rss:0kB, UID:0 pgtables:756kB oom_score_adj:984
[Dec 2 20:56] C2 CompilerThre invoked oom-killer: gfp_mask=0xcc0(GFP_KERNEL), order=0, oom_score_adj=984
[  +0.000007] CPU: 1 PID: 77236 Comm: C2 CompilerThre Not tainted 5.15.153.1-microsoft-standard-WSL2 #1
[  +0.000002] Call Trace:
[  +0.000003]  <TASK>
[  +0.000003]  dump_stack_lvl+0x34/0x48
[  +0.000005]  dump_header+0x4a/0x18b
[  +0.000003]  oom_kill_process.cold+0xb/0x10
[  +0.000002]  out_of_memory+0x1f7/0x2a0
[  +0.000003]  mem_cgroup_out_of_memory+0x13a/0x150
[  +0.000045]  try_charge_memcg+0x6f4/0x7b0
[  +0.000004]  charge_memcg+0x3f/0x90
[  +0.000002]  __mem_cgroup_charge+0x2c/0x90
[  +0.000002]  __handle_mm_fault+0x9b3/0x13b0
[  +0.000005]  handle_mm_fault+0xbf/0x290
[  +0.000003]  do_user_addr_fault+0x1b2/0x650
[  +0.000003]  exc_page_fault+0x5d/0x100
[  +0.000004]  asm_exc_page_fault+0x22/0x30
[  +0.000002] RIP: 0033:0x7f4833cdcc32
[  +0.000003] Code: 0f 1f 84 00 00 00 00 00 55 48 89 e5 41 55 41 54 44 8d a6 ff 00 00 00 53 41 c1 ec 08 48 89 fb 48 83 ec 08 48 c7 07 00 00 00 00 <44> 89 a7 90 00 00 00 41 83 fc 10 77 51 48 8d 47 10 48 89 47 08 45
[  +0.000023] RSP: 002b:00007f47d6befd20 EFLAGS: 00010202
[  +0.000002] RAX: 000000000000002c RBX: 00007f47d1b4fff0 RCX: 000000000000002b
[  +0.000002] RDX: 00007f47d199a510 RSI: 0000000000002b68 RDI: 00007f47d1b4fff0
[  +0.000002] RBP: 00007f47d6befd40 R08: 00007f47d4d17ee0 R09: 0000000000000000
[  +0.000002] R10: 0000000000000022 R11: 0000000000000246 R12: 000000000000002c
[  +0.000001] R13: 0000000a00000000 R14: 000000000017e300 R15: 00007f47d6befe50
[  +0.000004]  </TASK>
[  +0.000192] Memory cgroup out of memory: Killed process 77213 (java) total-vm:1818104kB, anon-rss:258224kB, file-rss:20844kB, shmem-rss:0kB, UID:0 pgtables:724kB oom_score_adj:984
[Dec 2 20:59] C1 CompilerThre invoked oom-killer: gfp_mask=0xcc0(GFP_KERNEL), order=0, oom_score_adj=984
[  +0.000007] CPU: 0 PID: 78078 Comm: C1 CompilerThre Not tainted 5.15.153.1-microsoft-standard-WSL2 #1
[  +0.000003] Call Trace:
[  +0.000004]  <TASK>
[  +0.000003]  dump_stack_lvl+0x34/0x48
[  +0.000007]  dump_header+0x4a/0x18b
[  +0.000004]  oom_kill_process.cold+0xb/0x10
[  +0.000002]  out_of_memory+0x1f7/0x2a0
[  +0.000005]  mem_cgroup_out_of_memory+0x13a/0x150
[  +0.000005]  try_charge_memcg+0x6f4/0x7b0
[  +0.000005]  ? __mod_memcg_lruvec_state+0x41/0x80
[  +0.000003]  charge_memcg+0x3f/0x90
[  +0.000004]  __mem_cgroup_charge+0x2c/0x90
[  +0.000003]  __handle_mm_fault+0x9b3/0x13b0
[  +0.000009]  handle_mm_fault+0xbf/0x290
[  +0.000003]  do_user_addr_fault+0x1b2/0x650
[  +0.000005]  exc_page_fault+0x5d/0x100
[  +0.000004]  asm_exc_page_fault+0x22/0x30
[  +0.000158] RIP: 0033:0x7f04b4e65e74
[  +0.000003] Code: 84 00 00 00 00 00 55 66 0f ef c0 48 89 e5 41 57 41 89 d7 41 56 41 55 41 54 49 89 fc 53 48 89 f3 48 83 ec 68 c7 07 00 00 00 00 <c7> 47 40 63 00 00 00 48 c7 47 48 ff ff ff ff 0f 11 47 30 66 48 8d
[  +0.000002] RSP: 002b:00007f0457efb5c0 EFLAGS: 00010206
[  +0.000002] RAX: 00007f045310afc0 RBX: 00007f04531077c0 RCX: 0000000000000006
[  +0.000001] RDX: 000000000000000c RSI: 00007f04531077c0 RDI: 00007f045310afc0
[  +0.000002] RBP: 00007f0457efb650 R08: 00007f0453109fe0 R09: 00007f045310ad80
[  +0.000001] R10: 00007f04b5c97a60 R11: 0000000000000206 R12: 00007f045310afc0
[  +0.000001] R13: 00007f04531077c0 R14: 000000000000000c R15: 000000000000000c
[  +0.000003]  </TASK>
[  +0.000228] Memory cgroup out of memory: Killed process 78056 (java) total-vm:1826076kB, anon-rss:256668kB, file-rss:20964kB, shmem-rss:0kB, UID:0 pgtables:748kB oom_score_adj:984


==> etcd [a480a505fe28] <==
{"level":"info","ts":"2025-12-01T23:26:17.496928Z","caller":"traceutil/trace.go:171","msg":"trace[115246933] transaction","detail":"{read_only:false; response_revision:12131; number_of_response:1; }","duration":"110.7676ms","start":"2025-12-01T23:26:17.386127Z","end":"2025-12-01T23:26:17.496895Z","steps":["trace[115246933] 'process raft request'  (duration: 110.529841ms)"],"step_count":1}
{"level":"info","ts":"2025-12-01T23:27:44.592639Z","caller":"traceutil/trace.go:171","msg":"trace[684182877] linearizableReadLoop","detail":"{readStateIndex:15207; appliedIndex:15206; }","duration":"142.470341ms","start":"2025-12-01T23:27:44.450144Z","end":"2025-12-01T23:27:44.592615Z","steps":["trace[684182877] 'read index received'  (duration: 94.827848ms)","trace[684182877] 'applied index is now lower than readState.Index'  (duration: 47.641593ms)"],"step_count":2}
{"level":"info","ts":"2025-12-01T23:27:44.592786Z","caller":"traceutil/trace.go:171","msg":"trace[1390014311] transaction","detail":"{read_only:false; response_revision:12201; number_of_response:1; }","duration":"162.126447ms","start":"2025-12-01T23:27:44.430628Z","end":"2025-12-01T23:27:44.592755Z","steps":["trace[1390014311] 'process raft request'  (duration: 114.273237ms)","trace[1390014311] 'compare'  (duration: 47.517982ms)"],"step_count":2}
{"level":"warn","ts":"2025-12-01T23:27:44.592844Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"142.763265ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" limit:1 ","response":"range_response_count:1 size:420"}
{"level":"info","ts":"2025-12-01T23:27:44.592961Z","caller":"traceutil/trace.go:171","msg":"trace[517333786] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:12201; }","duration":"142.905376ms","start":"2025-12-01T23:27:44.449979Z","end":"2025-12-01T23:27:44.592885Z","steps":["trace[517333786] 'agreement among raft nodes before linearized reading'  (duration: 142.781466ms)"],"step_count":1}
{"level":"info","ts":"2025-12-01T23:28:25.009347Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11994}
{"level":"info","ts":"2025-12-01T23:28:25.018030Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":11994,"took":"8.390965ms","hash":4199351019,"current-db-size-bytes":2428928,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1318912,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2025-12-01T23:28:25.018167Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4199351019,"revision":11994,"compact-revision":11761}
{"level":"info","ts":"2025-12-01T23:29:26.572575Z","caller":"traceutil/trace.go:171","msg":"trace[1364393067] transaction","detail":"{read_only:false; response_revision:12283; number_of_response:1; }","duration":"142.698609ms","start":"2025-12-01T23:29:26.429840Z","end":"2025-12-01T23:29:26.572538Z","steps":["trace[1364393067] 'process raft request'  (duration: 142.381586ms)"],"step_count":1}
{"level":"info","ts":"2025-12-01T23:30:18.319226Z","caller":"traceutil/trace.go:171","msg":"trace[1617762318] linearizableReadLoop","detail":"{readStateIndex:15361; appliedIndex:15360; }","duration":"101.795525ms","start":"2025-12-01T23:30:18.217386Z","end":"2025-12-01T23:30:18.319181Z","steps":["trace[1617762318] 'read index received'  (duration: 101.171579ms)","trace[1617762318] 'applied index is now lower than readState.Index'  (duration: 622.346¬µs)"],"step_count":2}
{"level":"info","ts":"2025-12-01T23:30:18.319458Z","caller":"traceutil/trace.go:171","msg":"trace[521967308] transaction","detail":"{read_only:false; response_revision:12324; number_of_response:1; }","duration":"111.793964ms","start":"2025-12-01T23:30:18.207624Z","end":"2025-12-01T23:30:18.319418Z","steps":["trace[521967308] 'process raft request'  (duration: 111.285927ms)"],"step_count":1}
{"level":"warn","ts":"2025-12-01T23:30:18.319552Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"102.131851ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-12-01T23:30:18.319662Z","caller":"traceutil/trace.go:171","msg":"trace[29155657] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:12324; }","duration":"102.25806ms","start":"2025-12-01T23:30:18.217368Z","end":"2025-12-01T23:30:18.319626Z","steps":["trace[29155657] 'agreement among raft nodes before linearized reading'  (duration: 102.106149ms)"],"step_count":1}
{"level":"info","ts":"2025-12-01T23:33:24.986899Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12233}
{"level":"info","ts":"2025-12-01T23:33:24.994176Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":12233,"took":"6.964506ms","hash":1911168136,"current-db-size-bytes":2428928,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1363968,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-12-01T23:33:24.994264Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1911168136,"revision":12233,"compact-revision":11994}
{"level":"warn","ts":"2025-12-01T23:33:49.349164Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"147.295496ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-12-01T23:33:49.349721Z","caller":"traceutil/trace.go:171","msg":"trace[510515208] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:12492; }","duration":"147.807204ms","start":"2025-12-01T23:33:49.201785Z","end":"2025-12-01T23:33:49.349593Z","steps":["trace[510515208] 'range keys from in-memory index tree'  (duration: 147.147182ms)"],"step_count":1}
{"level":"info","ts":"2025-12-01T23:38:19.320824Z","caller":"traceutil/trace.go:171","msg":"trace[1584300285] transaction","detail":"{read_only:false; response_revision:12706; number_of_response:1; }","duration":"115.535542ms","start":"2025-12-01T23:38:19.204930Z","end":"2025-12-01T23:38:19.320466Z","steps":["trace[1584300285] 'process raft request'  (duration: 115.353319ms)"],"step_count":1}
{"level":"info","ts":"2025-12-01T23:38:24.985342Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12472}
{"level":"info","ts":"2025-12-01T23:38:24.996097Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":12472,"took":"10.332719ms","hash":2869367503,"current-db-size-bytes":2428928,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1359872,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-12-01T23:38:24.996294Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2869367503,"revision":12472,"compact-revision":12233}
{"level":"info","ts":"2025-12-01T23:43:24.976933Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12710}
{"level":"info","ts":"2025-12-01T23:43:24.984731Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":12710,"took":"7.264302ms","hash":437104756,"current-db-size-bytes":2428928,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1384448,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-12-01T23:43:24.984823Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":437104756,"revision":12710,"compact-revision":12472}
{"level":"info","ts":"2025-12-01T23:48:24.972111Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12950}
{"level":"info","ts":"2025-12-01T23:48:24.979001Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":12950,"took":"6.405549ms","hash":1033204331,"current-db-size-bytes":2428928,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1359872,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-12-01T23:48:24.979140Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1033204331,"revision":12950,"compact-revision":12710}
{"level":"warn","ts":"2025-12-01T23:51:49.771236Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"397.091387ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128041690745918495 > lease_revoke:<id:70cc9ada8079d3d2>","response":"size:29"}
{"level":"info","ts":"2025-12-01T23:51:49.771493Z","caller":"traceutil/trace.go:171","msg":"trace[48125442] linearizableReadLoop","detail":"{readStateIndex:16651; appliedIndex:16650; }","duration":"348.945945ms","start":"2025-12-01T23:51:49.422516Z","end":"2025-12-01T23:51:49.771462Z","steps":["trace[48125442] 'read index received'  (duration: 72.605¬µs)","trace[48125442] 'applied index is now lower than readState.Index'  (duration: 348.869939ms)"],"step_count":2}
{"level":"warn","ts":"2025-12-01T23:51:49.771760Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"349.212564ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-12-01T23:51:49.771840Z","caller":"traceutil/trace.go:171","msg":"trace[532468933] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:13351; }","duration":"349.347174ms","start":"2025-12-01T23:51:49.422468Z","end":"2025-12-01T23:51:49.771815Z","steps":["trace[532468933] 'agreement among raft nodes before linearized reading'  (duration: 349.087455ms)"],"step_count":1}
{"level":"warn","ts":"2025-12-01T23:51:49.772050Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-12-01T23:51:49.422446Z","time spent":"349.43658ms","remote":"127.0.0.1:45666","response type":"/etcdserverpb.KV/Range","request count":0,"request size":69,"response count":1,"response size":1133,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 "}
{"level":"info","ts":"2025-12-01T23:51:56.339395Z","caller":"traceutil/trace.go:171","msg":"trace[1614655553] transaction","detail":"{read_only:false; response_revision:13358; number_of_response:1; }","duration":"198.333029ms","start":"2025-12-01T23:51:56.141027Z","end":"2025-12-01T23:51:56.339360Z","steps":["trace[1614655553] 'process raft request'  (duration: 198.084012ms)"],"step_count":1}
{"level":"warn","ts":"2025-12-01T23:51:59.511702Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"152.855135ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128041690745918543 > lease_revoke:<id:70cc9ada8079d403>","response":"size:29"}
{"level":"info","ts":"2025-12-01T23:52:02.477675Z","caller":"traceutil/trace.go:171","msg":"trace[969460492] transaction","detail":"{read_only:false; response_revision:13361; number_of_response:1; }","duration":"518.798537ms","start":"2025-12-01T23:52:01.958841Z","end":"2025-12-01T23:52:02.477639Z","steps":["trace[969460492] 'process raft request'  (duration: 518.655227ms)"],"step_count":1}
{"level":"warn","ts":"2025-12-01T23:52:02.478116Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"397.97465ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-12-01T23:52:02.478218Z","caller":"traceutil/trace.go:171","msg":"trace[848366535] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:13361; }","duration":"398.073857ms","start":"2025-12-01T23:52:02.080113Z","end":"2025-12-01T23:52:02.478187Z","steps":["trace[848366535] 'agreement among raft nodes before linearized reading'  (duration: 397.943248ms)"],"step_count":1}
{"level":"info","ts":"2025-12-01T23:52:02.477710Z","caller":"traceutil/trace.go:171","msg":"trace[1980641902] linearizableReadLoop","detail":"{readStateIndex:16663; appliedIndex:16663; }","duration":"397.554121ms","start":"2025-12-01T23:52:02.080128Z","end":"2025-12-01T23:52:02.477682Z","steps":["trace[1980641902] 'read index received'  (duration: 397.54572ms)","trace[1980641902] 'applied index is now lower than readState.Index'  (duration: 7.001¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-12-01T23:52:02.489026Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"385.469572ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-12-01T23:52:02.489179Z","caller":"traceutil/trace.go:171","msg":"trace[1807333486] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:13361; }","duration":"385.680687ms","start":"2025-12-01T23:52:02.103466Z","end":"2025-12-01T23:52:02.489147Z","steps":["trace[1807333486] 'agreement among raft nodes before linearized reading'  (duration: 385.469772ms)"],"step_count":1}
{"level":"warn","ts":"2025-12-01T23:52:02.489414Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-12-01T23:52:02.103436Z","time spent":"385.958706ms","remote":"127.0.0.1:45468","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-12-01T23:52:02.496357Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-12-01T23:52:01.958803Z","time spent":"518.974949ms","remote":"127.0.0.1:45666","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:13360 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2025-12-01T23:52:37.191065Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"110.50751ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-12-01T23:52:37.191293Z","caller":"traceutil/trace.go:171","msg":"trace[1328465978] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:13389; }","duration":"110.743029ms","start":"2025-12-01T23:52:37.080520Z","end":"2025-12-01T23:52:37.191263Z","steps":["trace[1328465978] 'range keys from in-memory index tree'  (duration: 110.489109ms)"],"step_count":1}
{"level":"info","ts":"2025-12-01T23:52:53.746232Z","caller":"traceutil/trace.go:171","msg":"trace[499155820] transaction","detail":"{read_only:false; response_revision:13401; number_of_response:1; }","duration":"119.453438ms","start":"2025-12-01T23:52:53.626749Z","end":"2025-12-01T23:52:53.746202Z","steps":["trace[499155820] 'process raft request'  (duration: 119.155717ms)"],"step_count":1}
{"level":"info","ts":"2025-12-01T23:52:59.694425Z","caller":"traceutil/trace.go:171","msg":"trace[1765279577] linearizableReadLoop","detail":"{readStateIndex:16720; appliedIndex:16719; }","duration":"187.869628ms","start":"2025-12-01T23:52:59.506518Z","end":"2025-12-01T23:52:59.694388Z","steps":["trace[1765279577] 'read index received'  (duration: 117.993133ms)","trace[1765279577] 'applied index is now lower than readState.Index'  (duration: 69.874295ms)"],"step_count":2}
{"level":"warn","ts":"2025-12-01T23:52:59.694607Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"188.097645ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/limitranges/\" range_end:\"/registry/limitranges0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-12-01T23:52:59.694688Z","caller":"traceutil/trace.go:171","msg":"trace[577761441] range","detail":"{range_begin:/registry/limitranges/; range_end:/registry/limitranges0; response_count:0; response_revision:13406; }","duration":"188.257256ms","start":"2025-12-01T23:52:59.506411Z","end":"2025-12-01T23:52:59.694669Z","steps":["trace[577761441] 'agreement among raft nodes before linearized reading'  (duration: 188.097744ms)"],"step_count":1}
{"level":"info","ts":"2025-12-01T23:52:59.997133Z","caller":"traceutil/trace.go:171","msg":"trace[124542162] transaction","detail":"{read_only:false; response_revision:13407; number_of_response:1; }","duration":"152.292489ms","start":"2025-12-01T23:52:59.844808Z","end":"2025-12-01T23:52:59.997100Z","steps":["trace[124542162] 'process raft request'  (duration: 152.082474ms)"],"step_count":1}
{"level":"info","ts":"2025-12-01T23:53:02.208755Z","caller":"traceutil/trace.go:171","msg":"trace[27121079] transaction","detail":"{read_only:false; response_revision:13408; number_of_response:1; }","duration":"198.978305ms","start":"2025-12-01T23:53:02.009750Z","end":"2025-12-01T23:53:02.208729Z","steps":["trace[27121079] 'process raft request'  (duration: 198.397765ms)"],"step_count":1}
{"level":"warn","ts":"2025-12-01T23:53:02.209063Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"130.861713ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-12-01T23:53:02.209145Z","caller":"traceutil/trace.go:171","msg":"trace[1143791508] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:13408; }","duration":"130.95882ms","start":"2025-12-01T23:53:02.078159Z","end":"2025-12-01T23:53:02.209118Z","steps":["trace[1143791508] 'agreement among raft nodes before linearized reading'  (duration: 130.834711ms)"],"step_count":1}
{"level":"info","ts":"2025-12-01T23:53:02.214672Z","caller":"traceutil/trace.go:171","msg":"trace[341052515] linearizableReadLoop","detail":"{readStateIndex:16722; appliedIndex:16721; }","duration":"130.169465ms","start":"2025-12-01T23:53:02.078169Z","end":"2025-12-01T23:53:02.208339Z","steps":["trace[341052515] 'read index received'  (duration: 129.81054ms)","trace[341052515] 'applied index is now lower than readState.Index'  (duration: 357.525¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-12-01T23:53:02.215130Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"112.390741ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-12-01T23:53:02.215268Z","caller":"traceutil/trace.go:171","msg":"trace[884921815] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:13408; }","duration":"112.578154ms","start":"2025-12-01T23:53:02.102660Z","end":"2025-12-01T23:53:02.215238Z","steps":["trace[884921815] 'agreement among raft nodes before linearized reading'  (duration: 112.384141ms)"],"step_count":1}
{"level":"info","ts":"2025-12-01T23:53:24.974678Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13189}
{"level":"info","ts":"2025-12-01T23:53:24.988789Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":13189,"took":"13.680968ms","hash":3289311494,"current-db-size-bytes":2428928,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1323008,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2025-12-01T23:53:24.988921Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3289311494,"revision":13189,"compact-revision":12950}
{"level":"info","ts":"2025-12-01T23:55:51.385398Z","caller":"traceutil/trace.go:171","msg":"trace[1111065281] transaction","detail":"{read_only:false; response_revision:13544; number_of_response:1; }","duration":"108.254851ms","start":"2025-12-01T23:55:51.277113Z","end":"2025-12-01T23:55:51.385368Z","steps":["trace[1111065281] 'process raft request'  (duration: 108.121442ms)"],"step_count":1}


==> etcd [f7c30c4f2878] <==
{"level":"info","ts":"2025-12-02T20:47:17.663866Z","caller":"traceutil/trace.go:171","msg":"trace[1854166272] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:29015; }","duration":"397.956798ms","start":"2025-12-02T20:47:17.265898Z","end":"2025-12-02T20:47:17.663855Z","steps":["trace[1854166272] 'agreement among raft nodes before linearized reading'  (duration: 397.858593ms)"],"step_count":1}
{"level":"warn","ts":"2025-12-02T20:47:17.664118Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"397.957997ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-12-02T20:47:17.664155Z","caller":"traceutil/trace.go:171","msg":"trace[1948077732] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:29015; }","duration":"398.0241ms","start":"2025-12-02T20:47:17.266120Z","end":"2025-12-02T20:47:17.664144Z","steps":["trace[1948077732] 'agreement among raft nodes before linearized reading'  (duration: 397.966297ms)"],"step_count":1}
{"level":"warn","ts":"2025-12-02T20:47:17.664191Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-12-02T20:47:17.266103Z","time spent":"398.078803ms","remote":"127.0.0.1:43112","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-12-02T20:47:18.161835Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"398.0175ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/default/kubernetes\" limit:1 ","response":"range_response_count:1 size:475"}
{"level":"info","ts":"2025-12-02T20:47:18.161911Z","caller":"traceutil/trace.go:171","msg":"trace[194149093] range","detail":"{range_begin:/registry/endpointslices/default/kubernetes; range_end:; response_count:1; response_revision:29015; }","duration":"398.134306ms","start":"2025-12-02T20:47:17.763758Z","end":"2025-12-02T20:47:18.161893Z","steps":["trace[194149093] 'range keys from in-memory index tree'  (duration: 397.913695ms)"],"step_count":1}
{"level":"warn","ts":"2025-12-02T20:47:18.161966Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-12-02T20:47:17.763738Z","time spent":"398.21271ms","remote":"127.0.0.1:43382","response type":"/etcdserverpb.KV/Range","request count":0,"request size":47,"response count":1,"response size":499,"request content":"key:\"/registry/endpointslices/default/kubernetes\" limit:1 "}
{"level":"info","ts":"2025-12-02T20:47:19.183482Z","caller":"traceutil/trace.go:171","msg":"trace[1340337146] transaction","detail":"{read_only:false; response_revision:29016; number_of_response:1; }","duration":"194.298661ms","start":"2025-12-02T20:47:18.989161Z","end":"2025-12-02T20:47:19.183460Z","steps":["trace[1340337146] 'process raft request'  (duration: 194.174855ms)"],"step_count":1}
{"level":"warn","ts":"2025-12-02T20:47:19.562157Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"300.053028ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-12-02T20:47:19.562233Z","caller":"traceutil/trace.go:171","msg":"trace[1212190120] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:29016; }","duration":"300.133232ms","start":"2025-12-02T20:47:19.262083Z","end":"2025-12-02T20:47:19.562216Z","steps":["trace[1212190120] 'range keys from in-memory index tree'  (duration: 299.965724ms)"],"step_count":1}
{"level":"warn","ts":"2025-12-02T20:47:23.416269Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"393.497547ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128041712478549814 > lease_revoke:<id:70cc9adf8fd776e0>","response":"size:30"}
{"level":"info","ts":"2025-12-02T20:47:23.416430Z","caller":"traceutil/trace.go:171","msg":"trace[1880670867] linearizableReadLoop","detail":"{readStateIndex:35853; appliedIndex:35852; }","duration":"652.1687ms","start":"2025-12-02T20:47:22.764242Z","end":"2025-12-02T20:47:23.416411Z","steps":["trace[1880670867] 'read index received'  (duration: 258.480167ms)","trace[1880670867] 'applied index is now lower than readState.Index'  (duration: 393.686632ms)"],"step_count":2}
{"level":"warn","ts":"2025-12-02T20:47:23.417045Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"652.839401ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-12-02T20:47:23.417096Z","caller":"traceutil/trace.go:171","msg":"trace[1026757729] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:29024; }","duration":"652.930842ms","start":"2025-12-02T20:47:22.764151Z","end":"2025-12-02T20:47:23.417082Z","steps":["trace[1026757729] 'agreement among raft nodes before linearized reading'  (duration: 652.848505ms)"],"step_count":1}
{"level":"warn","ts":"2025-12-02T20:47:23.417153Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-12-02T20:47:22.764130Z","time spent":"653.012779ms","remote":"127.0.0.1:43114","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-12-02T20:47:23.417476Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"531.096687ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/default/backend-manga2you\" limit:1 ","response":"range_response_count:1 size:2948"}
{"level":"info","ts":"2025-12-02T20:47:23.417514Z","caller":"traceutil/trace.go:171","msg":"trace[400335274] range","detail":"{range_begin:/registry/deployments/default/backend-manga2you; range_end:; response_count:1; response_revision:29024; }","duration":"531.17082ms","start":"2025-12-02T20:47:22.886332Z","end":"2025-12-02T20:47:23.417503Z","steps":["trace[400335274] 'agreement among raft nodes before linearized reading'  (duration: 531.035859ms)"],"step_count":1}
{"level":"warn","ts":"2025-12-02T20:47:23.417580Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-12-02T20:47:22.886312Z","time spent":"531.254958ms","remote":"127.0.0.1:43550","response type":"/etcdserverpb.KV/Range","request count":0,"request size":51,"response count":1,"response size":2972,"request content":"key:\"/registry/deployments/default/backend-manga2you\" limit:1 "}
{"level":"warn","ts":"2025-12-02T20:47:23.418039Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"150.166888ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-12-02T20:47:23.418076Z","caller":"traceutil/trace.go:171","msg":"trace[1105906542] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:29024; }","duration":"150.205606ms","start":"2025-12-02T20:47:23.267859Z","end":"2025-12-02T20:47:23.418065Z","steps":["trace[1105906542] 'agreement among raft nodes before linearized reading'  (duration: 150.152482ms)"],"step_count":1}
{"level":"warn","ts":"2025-12-02T20:47:24.378142Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"592.5375ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/\" range_end:\"/registry/deployments0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2025-12-02T20:47:24.378240Z","caller":"traceutil/trace.go:171","msg":"trace[545660145] range","detail":"{range_begin:/registry/deployments/; range_end:/registry/deployments0; response_count:0; response_revision:29026; }","duration":"592.681565ms","start":"2025-12-02T20:47:23.785544Z","end":"2025-12-02T20:47:24.378225Z","steps":["trace[545660145] 'count revisions from in-memory index tree'  (duration: 592.458464ms)"],"step_count":1}
{"level":"warn","ts":"2025-12-02T20:47:24.378293Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-12-02T20:47:23.785521Z","time spent":"592.752297ms","remote":"127.0.0.1:43550","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":5,"response size":32,"request content":"key:\"/registry/deployments/\" range_end:\"/registry/deployments0\" count_only:true "}
{"level":"info","ts":"2025-12-02T20:47:27.861873Z","caller":"traceutil/trace.go:171","msg":"trace[1202988281] transaction","detail":"{read_only:false; response_revision:29036; number_of_response:1; }","duration":"174.164074ms","start":"2025-12-02T20:47:27.687686Z","end":"2025-12-02T20:47:27.861851Z","steps":["trace[1202988281] 'process raft request'  (duration: 173.751588ms)"],"step_count":1}
{"level":"info","ts":"2025-12-02T20:47:28.229840Z","caller":"traceutil/trace.go:171","msg":"trace[2120745900] transaction","detail":"{read_only:false; response_revision:29037; number_of_response:1; }","duration":"228.734498ms","start":"2025-12-02T20:47:28.001078Z","end":"2025-12-02T20:47:28.229813Z","steps":["trace[2120745900] 'process raft request'  (duration: 227.151087ms)"],"step_count":1}
{"level":"info","ts":"2025-12-02T20:47:46.363876Z","caller":"traceutil/trace.go:171","msg":"trace[1630232683] linearizableReadLoop","detail":"{readStateIndex:35883; appliedIndex:35882; }","duration":"158.008216ms","start":"2025-12-02T20:47:46.205848Z","end":"2025-12-02T20:47:46.363856Z","steps":["trace[1630232683] 'read index received'  (duration: 157.922494ms)","trace[1630232683] 'applied index is now lower than readState.Index'  (duration: 84.922¬µs)"],"step_count":2}
{"level":"info","ts":"2025-12-02T20:47:46.364001Z","caller":"traceutil/trace.go:171","msg":"trace[668651033] transaction","detail":"{read_only:false; response_revision:29050; number_of_response:1; }","duration":"258.177527ms","start":"2025-12-02T20:47:46.105811Z","end":"2025-12-02T20:47:46.363989Z","steps":["trace[668651033] 'process raft request'  (duration: 257.936865ms)"],"step_count":1}
{"level":"warn","ts":"2025-12-02T20:47:46.375222Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"169.358441ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-12-02T20:47:46.376134Z","caller":"traceutil/trace.go:171","msg":"trace[554653524] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:29050; }","duration":"170.268175ms","start":"2025-12-02T20:47:46.205840Z","end":"2025-12-02T20:47:46.376108Z","steps":["trace[554653524] 'agreement among raft nodes before linearized reading'  (duration: 158.221771ms)"],"step_count":1}
{"level":"info","ts":"2025-12-02T20:47:54.680394Z","caller":"traceutil/trace.go:171","msg":"trace[1603390027] linearizableReadLoop","detail":"{readStateIndex:35892; appliedIndex:35891; }","duration":"188.572224ms","start":"2025-12-02T20:47:54.491794Z","end":"2025-12-02T20:47:54.680366Z","steps":["trace[1603390027] 'read index received'  (duration: 173.414119ms)","trace[1603390027] 'applied index is now lower than readState.Index'  (duration: 15.156405ms)"],"step_count":2}
{"level":"warn","ts":"2025-12-02T20:47:54.680543Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"188.72903ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/dev/dev-backend-manga2you-v1\" limit:1 ","response":"range_response_count:1 size:3567"}
{"level":"info","ts":"2025-12-02T20:47:54.680878Z","caller":"traceutil/trace.go:171","msg":"trace[1715786879] range","detail":"{range_begin:/registry/deployments/dev/dev-backend-manga2you-v1; range_end:; response_count:1; response_revision:29057; }","duration":"189.102445ms","start":"2025-12-02T20:47:54.491760Z","end":"2025-12-02T20:47:54.680862Z","steps":["trace[1715786879] 'agreement among raft nodes before linearized reading'  (duration: 188.71333ms)"],"step_count":1}
{"level":"info","ts":"2025-12-02T20:47:54.681677Z","caller":"traceutil/trace.go:171","msg":"trace[2066486504] transaction","detail":"{read_only:false; response_revision:29057; number_of_response:1; }","duration":"188.820434ms","start":"2025-12-02T20:47:54.491748Z","end":"2025-12-02T20:47:54.680569Z","steps":["trace[2066486504] 'process raft request'  (duration: 173.510823ms)","trace[2066486504] 'store kv pair into bolt db' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1090; } (duration: 14.954797ms)"],"step_count":2}
{"level":"info","ts":"2025-12-02T20:49:57.758211Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":28886}
{"level":"info","ts":"2025-12-02T20:49:57.764133Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":28886,"took":"5.60701ms","hash":795602416,"current-db-size-bytes":2977792,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":2437120,"current-db-size-in-use":"2.4 MB"}
{"level":"info","ts":"2025-12-02T20:49:57.764202Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":795602416,"revision":28886,"compact-revision":28507}
{"level":"info","ts":"2025-12-02T20:54:57.739730Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":29194}
{"level":"info","ts":"2025-12-02T20:54:57.744955Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":29194,"took":"4.9738ms","hash":3730762385,"current-db-size-bytes":2977792,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":2199552,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-12-02T20:54:57.745013Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3730762385,"revision":29194,"compact-revision":28886}
{"level":"info","ts":"2025-12-02T20:59:57.723859Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":29493}
{"level":"info","ts":"2025-12-02T20:59:57.731174Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":29493,"took":"7.070663ms","hash":2279362162,"current-db-size-bytes":2977792,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":2048000,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-12-02T20:59:57.731245Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2279362162,"revision":29493,"compact-revision":29194}
{"level":"warn","ts":"2025-12-02T21:01:46.228493Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.942485ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-12-02T21:01:46.228626Z","caller":"traceutil/trace.go:171","msg":"trace[222427575] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:29871; }","duration":"106.089792ms","start":"2025-12-02T21:01:46.122515Z","end":"2025-12-02T21:01:46.228604Z","steps":["trace[222427575] 'range keys from in-memory index tree'  (duration: 105.85928ms)"],"step_count":1}
{"level":"info","ts":"2025-12-02T21:04:57.727192Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":29782}
{"level":"info","ts":"2025-12-02T21:04:57.732235Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":29782,"took":"4.791176ms","hash":3504057781,"current-db-size-bytes":2977792,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1941504,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-12-02T21:04:57.732288Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3504057781,"revision":29782,"compact-revision":29493}
{"level":"info","ts":"2025-12-02T21:09:57.721619Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":30054}
{"level":"info","ts":"2025-12-02T21:09:57.727408Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":30054,"took":"5.272481ms","hash":1789222938,"current-db-size-bytes":2977792,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1941504,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-12-02T21:09:57.727502Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1789222938,"revision":30054,"compact-revision":29782}
{"level":"info","ts":"2025-12-02T21:14:57.716861Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":30318}
{"level":"info","ts":"2025-12-02T21:14:57.721188Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":30318,"took":"4.119008ms","hash":2805842762,"current-db-size-bytes":2977792,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1875968,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-12-02T21:14:57.721233Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2805842762,"revision":30318,"compact-revision":30054}
{"level":"info","ts":"2025-12-02T21:19:57.711870Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":30562}
{"level":"info","ts":"2025-12-02T21:19:57.717060Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":30562,"took":"4.885046ms","hash":942749167,"current-db-size-bytes":2977792,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1773568,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-12-02T21:19:57.717105Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":942749167,"revision":30562,"compact-revision":30318}
{"level":"info","ts":"2025-12-02T21:22:29.254367Z","caller":"traceutil/trace.go:171","msg":"trace[891190475] transaction","detail":"{read_only:false; response_revision:30964; number_of_response:1; }","duration":"141.552467ms","start":"2025-12-02T21:22:29.112789Z","end":"2025-12-02T21:22:29.254341Z","steps":["trace[891190475] 'process raft request'  (duration: 141.320756ms)"],"step_count":1}
{"level":"info","ts":"2025-12-02T21:24:57.707416Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":30808}
{"level":"info","ts":"2025-12-02T21:24:57.711239Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":30808,"took":"3.593565ms","hash":3779854347,"current-db-size-bytes":2977792,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":2097152,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-12-02T21:24:57.711292Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3779854347,"revision":30808,"compact-revision":30562}


==> kernel <==
 21:25:45 up  5:18,  0 users,  load average: 0.22, 0.26, 0.35
Linux minikube 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [66eb19b9895f] <==
I1201 15:20:48.970243       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1201 15:20:48.970338       1 local_available_controller.go:156] Starting LocalAvailability controller
I1201 15:20:48.970388       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1201 15:20:48.970394       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1201 15:20:48.970424       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1201 15:20:48.970577       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1201 15:20:48.970639       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1201 15:20:48.987468       1 aggregator.go:169] waiting for initial CRD sync...
I1201 15:20:48.987559       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1201 15:20:48.987576       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1201 15:20:48.987577       1 controller.go:119] Starting legacy_token_tracking_controller
I1201 15:20:48.987707       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1201 15:20:48.987763       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1201 15:20:48.987817       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1201 15:20:48.988284       1 controller.go:142] Starting OpenAPI controller
I1201 15:20:48.988343       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I1201 15:20:48.988354       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1201 15:20:48.988371       1 controller.go:78] Starting OpenAPI AggregationController
I1201 15:20:48.988393       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1201 15:20:48.988840       1 establishing_controller.go:81] Starting EstablishingController
I1201 15:20:48.988899       1 controller.go:90] Starting OpenAPI V3 controller
I1201 15:20:48.988915       1 naming_controller.go:294] Starting NamingConditionController
I1201 15:20:48.988931       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1201 15:20:48.988947       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1201 15:20:48.988977       1 crd_finalizer.go:269] Starting CRDFinalizer
I1201 15:20:49.006109       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I1201 15:20:49.006140       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1201 15:20:49.006181       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1201 15:20:49.006285       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1201 15:20:49.049708       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1201 15:20:49.240817       1 shared_informer.go:320] Caches are synced for node_authorizer
I1201 15:20:49.244670       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1201 15:20:49.244675       1 shared_informer.go:320] Caches are synced for configmaps
I1201 15:20:49.244678       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1201 15:20:49.245247       1 aggregator.go:171] initial CRD sync complete...
I1201 15:20:49.246365       1 autoregister_controller.go:144] Starting autoregister controller
I1201 15:20:49.246642       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1201 15:20:49.246826       1 cache.go:39] Caches are synced for autoregister controller
I1201 15:20:49.257361       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1201 15:20:49.257802       1 policy_source.go:240] refreshing policies
I1201 15:20:49.259482       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1201 15:20:49.336210       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1201 15:20:49.336308       1 cache.go:39] Caches are synced for LocalAvailability controller
I1201 15:20:49.336842       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1201 15:20:49.336912       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1201 15:20:49.336930       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1201 15:20:49.350980       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I1201 15:20:49.767602       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I1201 15:20:50.037743       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1201 15:20:50.954306       1 controller.go:615] quota admission added evaluator for: endpoints
I1201 15:20:52.762669       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1201 15:20:52.813397       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I1201 15:20:52.917935       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I1201 15:20:53.023435       1 controller.go:615] quota admission added evaluator for: deployments.apps
E1201 16:41:27.020011       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1201 16:41:27.021981       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1201 19:18:33.882553       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1201 19:18:33.942759       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1201 21:22:35.891203       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1201 21:22:35.919319       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"


==> kube-apiserver [ea4c0cf47fcc] <==
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I1202 19:08:07.344339       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I1202 19:08:07.344423       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1202 19:10:07.336641       1 handler_proxy.go:99] no RequestInfo found in the context
W1202 19:10:07.336770       1 handler_proxy.go:99] no RequestInfo found in the context
E1202 19:10:07.336823       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
E1202 19:10:07.336885       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I1202 19:10:07.338166       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I1202 19:10:07.338239       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1202 19:11:46.974404       1 handler_proxy.go:99] no RequestInfo found in the context
E1202 19:11:46.974625       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
W1202 19:11:47.977342       1 handler_proxy.go:99] no RequestInfo found in the context
E1202 19:11:47.977468       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
W1202 19:11:47.977505       1 handler_proxy.go:99] no RequestInfo found in the context
E1202 19:11:47.977671       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I1202 19:11:47.978830       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I1202 19:11:47.978911       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1202 19:12:47.975390       1 handler_proxy.go:99] no RequestInfo found in the context
W1202 19:12:47.975511       1 handler_proxy.go:99] no RequestInfo found in the context
E1202 19:12:47.975540       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E1202 19:12:47.975558       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I1202 19:12:47.976880       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I1202 19:12:47.977143       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E1202 19:14:21.803567       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.111.101.33:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.111.101.33:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.111.101.33:443: connect: connection refused" logger="UnhandledError"
W1202 19:14:21.803894       1 handler_proxy.go:99] no RequestInfo found in the context
E1202 19:14:21.804172       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E1202 19:14:21.806920       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.111.101.33:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.111.101.33:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.111.101.33:443: connect: connection refused" logger="UnhandledError"
E1202 19:14:21.812423       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.111.101.33:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.111.101.33:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.111.101.33:443: connect: connection refused" logger="UnhandledError"
I1202 19:14:22.151754       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
E1202 19:46:56.984888       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1202 19:46:56.986115       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1202 19:50:06.773621       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1202 19:50:06.774134       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1202 19:50:06.774247       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1202 19:50:07.374396       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I1202 20:38:36.499354       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1202 21:22:09.904559       1 handler.go:286] Adding GroupVersion crd.k8s.amazonaws.com v1alpha1 to ResourceManager
I1202 21:22:09.938046       1 handler.go:286] Adding GroupVersion networking.k8s.aws v1alpha1 to ResourceManager
I1202 21:22:10.029547       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I1202 21:25:06.827421       1 handler.go:286] Adding GroupVersion crd.k8s.amazonaws.com v1alpha1 to ResourceManager
I1202 21:25:06.848984       1 handler.go:286] Adding GroupVersion networking.k8s.aws v1alpha1 to ResourceManager
W1202 21:25:07.883933       1 cacher.go:171] Terminating all watchers from cacher policyendpoints.networking.k8s.aws
W1202 21:25:07.953178       1 cacher.go:171] Terminating all watchers from cacher eniconfigs.crd.k8s.amazonaws.com


==> kube-controller-manager [63111d623a0d] <==
E1202 21:24:40.345415       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/backend-manga2you: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container backend-manga2you-container of Pod backend-manga2you-575c99d7fb-nbr4p" logger="UnhandledError"
W1202 21:24:41.495678       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "dev"
W1202 21:24:55.350665       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E1202 21:24:55.356204       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/backend-manga2you: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container backend-manga2you-container of Pod backend-manga2you-575c99d7fb-nbr4p" logger="UnhandledError"
W1202 21:24:56.506958       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "dev"
E1202 21:25:07.885598       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: the server could not find the requested resource" logger="UnhandledError"
E1202 21:25:07.954486       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: the server could not find the requested resource" logger="UnhandledError"
W1202 21:25:09.248901       1 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = the server could not find the requested resource
E1202 21:25:09.250958       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="the server could not find the requested resource" resource="crd.k8s.amazonaws.com/v1alpha1, Resource=eniconfigs"
W1202 21:25:09.251849       1 reflector.go:569] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E1202 21:25:09.252592       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource" logger="UnhandledError"
W1202 21:25:09.303788       1 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = the server could not find the requested resource
E1202 21:25:09.304829       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="the server could not find the requested resource" resource="networking.k8s.aws/v1alpha1, Resource=policyendpoints"
W1202 21:25:09.305716       1 reflector.go:569] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E1202 21:25:09.305756       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource" logger="UnhandledError"
W1202 21:25:10.362312       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E1202 21:25:10.366152       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/backend-manga2you: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container backend-manga2you-container of Pod backend-manga2you-575c99d7fb-nbr4p" logger="UnhandledError"
W1202 21:25:11.361016       1 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = the server could not find the requested resource
E1202 21:25:11.362430       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="the server could not find the requested resource" resource="networking.k8s.aws/v1alpha1, Resource=policyendpoints"
W1202 21:25:11.363976       1 reflector.go:569] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E1202 21:25:11.364053       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource" logger="UnhandledError"
W1202 21:25:11.515090       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "dev"
W1202 21:25:12.336681       1 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = the server could not find the requested resource
E1202 21:25:12.338605       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="the server could not find the requested resource" resource="crd.k8s.amazonaws.com/v1alpha1, Resource=eniconfigs"
W1202 21:25:12.340276       1 reflector.go:569] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E1202 21:25:12.340362       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource" logger="UnhandledError"
W1202 21:25:15.949493       1 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = the server could not find the requested resource
E1202 21:25:15.950454       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="the server could not find the requested resource" resource="crd.k8s.amazonaws.com/v1alpha1, Resource=eniconfigs"
W1202 21:25:15.951266       1 reflector.go:569] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E1202 21:25:15.951305       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource" logger="UnhandledError"
W1202 21:25:16.129680       1 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = the server could not find the requested resource
E1202 21:25:16.131730       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="the server could not find the requested resource" resource="networking.k8s.aws/v1alpha1, Resource=policyendpoints"
W1202 21:25:16.132975       1 reflector.go:569] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E1202 21:25:16.133051       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource" logger="UnhandledError"
W1202 21:25:25.183273       1 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = the server could not find the requested resource
E1202 21:25:25.185044       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="the server could not find the requested resource" resource="networking.k8s.aws/v1alpha1, Resource=policyendpoints"
W1202 21:25:25.186279       1 reflector.go:569] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E1202 21:25:25.186337       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource" logger="UnhandledError"
W1202 21:25:25.371220       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E1202 21:25:25.375453       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/backend-manga2you: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container backend-manga2you-container of Pod backend-manga2you-575c99d7fb-nbr4p" logger="UnhandledError"
W1202 21:25:26.476623       1 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = the server could not find the requested resource
E1202 21:25:26.477762       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="the server could not find the requested resource" resource="crd.k8s.amazonaws.com/v1alpha1, Resource=eniconfigs"
W1202 21:25:26.478803       1 reflector.go:569] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E1202 21:25:26.478856       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource" logger="UnhandledError"
W1202 21:25:26.525602       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "dev"
I1202 21:25:31.026088       1 shared_informer.go:313] Waiting for caches to sync for resource quota
I1202 21:25:31.026149       1 shared_informer.go:320] Caches are synced for resource quota
I1202 21:25:32.025792       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I1202 21:25:32.025866       1 shared_informer.go:320] Caches are synced for garbage collector
W1202 21:25:39.847911       1 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = the server could not find the requested resource
E1202 21:25:39.848887       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="the server could not find the requested resource" resource="networking.k8s.aws/v1alpha1, Resource=policyendpoints"
W1202 21:25:39.849743       1 reflector.go:569] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E1202 21:25:39.849780       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource" logger="UnhandledError"
W1202 21:25:40.384465       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "default"
E1202 21:25:40.388711       1 horizontal.go:275] "Unhandled Error" err="failed to compute desired number of replicas based on listed metrics for Deployment/default/backend-manga2you: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container backend-manga2you-container of Pod backend-manga2you-575c99d7fb-nbr4p" logger="UnhandledError"
W1202 21:25:41.535741       1 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = pods.metrics.k8s.io is forbidden: User "system:serviceaccount:kube-system:horizontal-pod-autoscaler" cannot watch resource "pods" in API group "metrics.k8s.io" in the namespace "dev"
W1202 21:25:45.587579       1 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = the server could not find the requested resource
E1202 21:25:45.588862       1 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="the server could not find the requested resource" resource="crd.k8s.amazonaws.com/v1alpha1, Resource=eniconfigs"
W1202 21:25:45.590210       1 reflector.go:569] k8s.io/client-go/metadata/metadatainformer/informer.go:138: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource
E1202 21:25:45.590270       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/metadata/metadatainformer/informer.go:138: Failed to watch *v1.PartialObjectMetadata: failed to list *v1.PartialObjectMetadata: the server could not find the requested resource" logger="UnhandledError"


==> kube-controller-manager [e7467ac06a70] <==
I1201 15:21:24.303168       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="412.221¬µs"
I1201 15:26:26.811955       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 15:31:32.394842       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 16:41:27.023751       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1201 16:41:27.023879       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1201 16:45:12.266129       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 16:50:18.705357       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 16:55:24.582465       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 17:00:31.182086       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 17:05:37.089372       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 17:10:43.256809       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 17:15:49.040230       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 17:20:55.423084       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 17:26:00.353317       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 17:29:26.215712       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." logger="certificatesigningrequest-cleaner-controller" csr="csr-s4ws5" approvedExpiration="1h0m0s"
I1201 17:31:06.669712       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 17:36:12.544629       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 17:41:18.831891       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 17:46:24.397901       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 17:51:30.678125       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 17:56:36.240121       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 18:01:41.713267       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 19:18:33.887828       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1201 19:18:33.947179       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1201 19:21:49.352824       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 19:26:55.536168       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 19:32:01.399348       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 19:37:07.524151       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 19:42:13.406096       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 19:47:19.288121       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
E1201 21:22:35.895990       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1201 21:22:35.921025       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I1201 21:24:54.794995       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 21:30:00.882394       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 21:35:07.054821       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 21:40:13.059704       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 21:45:18.834822       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 21:50:24.614040       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 21:55:30.372881       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 22:00:36.302161       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 22:05:41.861256       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 22:10:47.924260       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 22:15:54.430117       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 22:20:59.917787       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 22:26:06.544785       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 22:31:11.684968       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 22:36:16.687559       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 22:41:22.383342       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 22:46:27.810669       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 22:51:33.548117       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 22:56:39.824375       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 23:01:45.385881       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 23:06:50.893410       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 23:23:27.394488       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 23:28:33.275700       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 23:33:38.548159       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 23:38:44.633537       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 23:43:51.318346       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 23:48:56.861765       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1201 23:54:02.235584       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [3b1b5163e4ea] <==
I1202 14:55:51.406632       1 server_linux.go:66] "Using iptables proxy"
I1202 14:55:51.830547       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1202 14:55:51.830829       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1202 14:55:51.895876       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1202 14:55:51.896040       1 server_linux.go:170] "Using iptables Proxier"
I1202 14:55:51.901641       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E1202 14:55:51.920171       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E1202 14:55:51.940742       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I1202 14:55:51.941997       1 server.go:497] "Version info" version="v1.32.0"
I1202 14:55:51.942103       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E1202 14:55:51.962414       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E1202 14:55:51.983876       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I1202 14:55:51.989678       1 config.go:329] "Starting node config controller"
I1202 14:55:51.989740       1 config.go:105] "Starting endpoint slice config controller"
I1202 14:55:51.989988       1 config.go:199] "Starting service config controller"
I1202 14:55:51.992641       1 shared_informer.go:313] Waiting for caches to sync for node config
I1202 14:55:51.992648       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1202 14:55:51.992652       1 shared_informer.go:313] Waiting for caches to sync for service config
I1202 14:55:52.093479       1 shared_informer.go:320] Caches are synced for service config
I1202 14:55:52.093576       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1202 14:55:52.093579       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [d8b66790a4c1] <==
I1201 15:20:52.250595       1 server_linux.go:66] "Using iptables proxy"
I1201 15:20:52.558331       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1201 15:20:52.558479       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1201 15:20:52.662078       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1201 15:20:52.662156       1 server_linux.go:170] "Using iptables Proxier"
I1201 15:20:52.665020       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E1201 15:20:52.675859       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E1201 15:20:52.684789       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I1201 15:20:52.684919       1 server.go:497] "Version info" version="v1.32.0"
I1201 15:20:52.684932       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E1201 15:20:52.692545       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E1201 15:20:52.703223       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I1201 15:20:52.704795       1 config.go:105] "Starting endpoint slice config controller"
I1201 15:20:52.704843       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1201 15:20:52.704912       1 config.go:199] "Starting service config controller"
I1201 15:20:52.704923       1 shared_informer.go:313] Waiting for caches to sync for service config
I1201 15:20:52.704969       1 config.go:329] "Starting node config controller"
I1201 15:20:52.705222       1 shared_informer.go:313] Waiting for caches to sync for node config
I1201 15:20:52.805802       1 shared_informer.go:320] Caches are synced for service config
I1201 15:20:52.805837       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1201 15:20:52.805883       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [6e0ad7990fd2] <==
I1202 14:55:39.998090       1 serving.go:386] Generated self-signed cert in-memory
W1202 14:55:45.095539       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1202 14:55:45.095652       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1202 14:55:45.095676       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1202 14:55:45.095691       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1202 14:55:45.485521       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I1202 14:55:45.485582       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1202 14:55:45.497446       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1202 14:55:45.497633       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1202 14:55:45.578424       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1202 14:55:45.578622       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1202 14:55:45.704640       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [ddc2c36144b9] <==
I1201 15:20:48.060146       1 serving.go:386] Generated self-signed cert in-memory
I1201 15:20:49.350069       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I1201 15:20:49.350266       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1201 15:20:49.436933       1 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
I1201 15:20:49.436956       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I1201 15:20:49.437005       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I1201 15:20:49.437009       1 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController
I1201 15:20:49.436932       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1201 15:20:49.437055       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1201 15:20:49.437179       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1201 15:20:49.437415       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1201 15:20:49.537146       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I1201 15:20:49.537193       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1201 15:20:49.537260       1 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController


==> kubelet <==
Dec 02 21:22:10 minikube kubelet[1667]: I1202 21:22:10.193573    1667 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-xtables-lock\") pod \"aws-node-s8rqs\" (UID: \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\") " pod="kube-system/aws-node-s8rqs"
Dec 02 21:22:10 minikube kubelet[1667]: I1202 21:22:10.193635    1667 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-net-dir\" (UniqueName: \"kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-cni-net-dir\") pod \"aws-node-s8rqs\" (UID: \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\") " pod="kube-system/aws-node-s8rqs"
Dec 02 21:22:10 minikube kubelet[1667]: I1202 21:22:10.193673    1667 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"run-dir\" (UniqueName: \"kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-run-dir\") pod \"aws-node-s8rqs\" (UID: \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\") " pod="kube-system/aws-node-s8rqs"
Dec 02 21:22:10 minikube kubelet[1667]: I1202 21:22:10.193795    1667 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"log-dir\" (UniqueName: \"kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-log-dir\") pod \"aws-node-s8rqs\" (UID: \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\") " pod="kube-system/aws-node-s8rqs"
Dec 02 21:22:10 minikube kubelet[1667]: I1202 21:22:10.193886    1667 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-hsgsx\" (UniqueName: \"kubernetes.io/projected/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-kube-api-access-hsgsx\") pod \"aws-node-s8rqs\" (UID: \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\") " pod="kube-system/aws-node-s8rqs"
Dec 02 21:22:10 minikube kubelet[1667]: I1202 21:22:10.193917    1667 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-bin-dir\" (UniqueName: \"kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-cni-bin-dir\") pod \"aws-node-s8rqs\" (UID: \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\") " pod="kube-system/aws-node-s8rqs"
Dec 02 21:22:10 minikube kubelet[1667]: I1202 21:22:10.193938    1667 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"bpf-pin-path\" (UniqueName: \"kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-bpf-pin-path\") pod \"aws-node-s8rqs\" (UID: \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\") " pod="kube-system/aws-node-s8rqs"
Dec 02 21:22:10 minikube kubelet[1667]: I1202 21:22:10.626956    1667 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="48ae625552e58673e019696823e0eb0e5b8e42a8b4d027de11b0470e20351c24"
Dec 02 21:22:11 minikube kubelet[1667]: E1202 21:22:11.674583    1667 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials" image="602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5"
Dec 02 21:22:11 minikube kubelet[1667]: E1202 21:22:11.675180    1667 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials" image="602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5"
Dec 02 21:22:11 minikube kubelet[1667]: E1202 21:22:11.678116    1667 kuberuntime_manager.go:1341] "Unhandled Error" err="init container &Container{Name:aws-vpc-cni-init,Image:602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:DISABLE_TCP_EARLY_DEMUX,Value:false,ValueFrom:nil,},EnvVar{Name:ENABLE_IPv6,Value:false,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:cni-bin-dir,ReadOnly:false,MountPath:/host/opt/cni/bin,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-hsgsx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod aws-node-s8rqs_kube-system(410733f0-3b24-4a0f-8eb5-9ed9ce66cc86): ErrImagePull: Error response from daemon: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials" logger="UnhandledError"
Dec 02 21:22:11 minikube kubelet[1667]: E1202 21:22:11.679557    1667 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"aws-vpc-cni-init\" with ErrImagePull: \"Error response from daemon: Head \\\"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\\\": no basic auth credentials\"" pod="kube-system/aws-node-s8rqs" podUID="410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"
Dec 02 21:22:12 minikube kubelet[1667]: E1202 21:22:12.645531    1667 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"aws-vpc-cni-init\" with ImagePullBackOff: \"Back-off pulling image \\\"602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5\\\": ErrImagePull: Error response from daemon: Head \\\"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\\\": no basic auth credentials\"" pod="kube-system/aws-node-s8rqs" podUID="410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"
Dec 02 21:22:28 minikube kubelet[1667]: E1202 21:22:28.075869    1667 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials" image="602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5"
Dec 02 21:22:28 minikube kubelet[1667]: E1202 21:22:28.075928    1667 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials" image="602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5"
Dec 02 21:22:28 minikube kubelet[1667]: E1202 21:22:28.076027    1667 kuberuntime_manager.go:1341] "Unhandled Error" err="init container &Container{Name:aws-vpc-cni-init,Image:602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:DISABLE_TCP_EARLY_DEMUX,Value:false,ValueFrom:nil,},EnvVar{Name:ENABLE_IPv6,Value:false,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:cni-bin-dir,ReadOnly:false,MountPath:/host/opt/cni/bin,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-hsgsx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod aws-node-s8rqs_kube-system(410733f0-3b24-4a0f-8eb5-9ed9ce66cc86): ErrImagePull: Error response from daemon: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials" logger="UnhandledError"
Dec 02 21:22:28 minikube kubelet[1667]: E1202 21:22:28.077297    1667 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"aws-vpc-cni-init\" with ErrImagePull: \"Error response from daemon: Head \\\"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\\\": no basic auth credentials\"" pod="kube-system/aws-node-s8rqs" podUID="410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"
Dec 02 21:22:41 minikube kubelet[1667]: E1202 21:22:41.260122    1667 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"aws-vpc-cni-init\" with ImagePullBackOff: \"Back-off pulling image \\\"602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5\\\": ErrImagePull: Error response from daemon: Head \\\"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\\\": no basic auth credentials\"" pod="kube-system/aws-node-s8rqs" podUID="410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"
Dec 02 21:22:54 minikube kubelet[1667]: E1202 21:22:54.108925    1667 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials" image="602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5"
Dec 02 21:22:54 minikube kubelet[1667]: E1202 21:22:54.109039    1667 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials" image="602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5"
Dec 02 21:22:54 minikube kubelet[1667]: E1202 21:22:54.109291    1667 kuberuntime_manager.go:1341] "Unhandled Error" err="init container &Container{Name:aws-vpc-cni-init,Image:602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:DISABLE_TCP_EARLY_DEMUX,Value:false,ValueFrom:nil,},EnvVar{Name:ENABLE_IPv6,Value:false,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:cni-bin-dir,ReadOnly:false,MountPath:/host/opt/cni/bin,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-hsgsx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod aws-node-s8rqs_kube-system(410733f0-3b24-4a0f-8eb5-9ed9ce66cc86): ErrImagePull: Error response from daemon: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials" logger="UnhandledError"
Dec 02 21:22:54 minikube kubelet[1667]: E1202 21:22:54.110603    1667 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"aws-vpc-cni-init\" with ErrImagePull: \"Error response from daemon: Head \\\"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\\\": no basic auth credentials\"" pod="kube-system/aws-node-s8rqs" podUID="410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"
Dec 02 21:23:06 minikube kubelet[1667]: E1202 21:23:06.257923    1667 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"aws-vpc-cni-init\" with ImagePullBackOff: \"Back-off pulling image \\\"602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5\\\": ErrImagePull: Error response from daemon: Head \\\"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\\\": no basic auth credentials\"" pod="kube-system/aws-node-s8rqs" podUID="410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"
Dec 02 21:23:21 minikube kubelet[1667]: E1202 21:23:21.256188    1667 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"aws-vpc-cni-init\" with ImagePullBackOff: \"Back-off pulling image \\\"602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5\\\": ErrImagePull: Error response from daemon: Head \\\"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\\\": no basic auth credentials\"" pod="kube-system/aws-node-s8rqs" podUID="410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"
Dec 02 21:23:37 minikube kubelet[1667]: E1202 21:23:37.168376    1667 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials" image="602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5"
Dec 02 21:23:37 minikube kubelet[1667]: E1202 21:23:37.168463    1667 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials" image="602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5"
Dec 02 21:23:37 minikube kubelet[1667]: E1202 21:23:37.168750    1667 kuberuntime_manager.go:1341] "Unhandled Error" err="init container &Container{Name:aws-vpc-cni-init,Image:602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:DISABLE_TCP_EARLY_DEMUX,Value:false,ValueFrom:nil,},EnvVar{Name:ENABLE_IPv6,Value:false,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:cni-bin-dir,ReadOnly:false,MountPath:/host/opt/cni/bin,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-hsgsx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod aws-node-s8rqs_kube-system(410733f0-3b24-4a0f-8eb5-9ed9ce66cc86): ErrImagePull: Error response from daemon: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials" logger="UnhandledError"
Dec 02 21:23:37 minikube kubelet[1667]: E1202 21:23:37.170142    1667 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"aws-vpc-cni-init\" with ErrImagePull: \"Error response from daemon: Head \\\"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\\\": no basic auth credentials\"" pod="kube-system/aws-node-s8rqs" podUID="410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"
Dec 02 21:23:49 minikube kubelet[1667]: E1202 21:23:49.256181    1667 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"aws-vpc-cni-init\" with ImagePullBackOff: \"Back-off pulling image \\\"602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5\\\": ErrImagePull: Error response from daemon: Head \\\"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\\\": no basic auth credentials\"" pod="kube-system/aws-node-s8rqs" podUID="410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"
Dec 02 21:24:02 minikube kubelet[1667]: E1202 21:24:02.255754    1667 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"aws-vpc-cni-init\" with ImagePullBackOff: \"Back-off pulling image \\\"602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5\\\": ErrImagePull: Error response from daemon: Head \\\"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\\\": no basic auth credentials\"" pod="kube-system/aws-node-s8rqs" podUID="410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"
Dec 02 21:24:16 minikube kubelet[1667]: E1202 21:24:16.254972    1667 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"aws-vpc-cni-init\" with ImagePullBackOff: \"Back-off pulling image \\\"602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5\\\": ErrImagePull: Error response from daemon: Head \\\"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\\\": no basic auth credentials\"" pod="kube-system/aws-node-s8rqs" podUID="410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"
Dec 02 21:24:27 minikube kubelet[1667]: E1202 21:24:27.254080    1667 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"aws-vpc-cni-init\" with ImagePullBackOff: \"Back-off pulling image \\\"602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5\\\": ErrImagePull: Error response from daemon: Head \\\"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\\\": no basic auth credentials\"" pod="kube-system/aws-node-s8rqs" podUID="410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"
Dec 02 21:24:39 minikube kubelet[1667]: E1202 21:24:39.254283    1667 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"aws-vpc-cni-init\" with ImagePullBackOff: \"Back-off pulling image \\\"602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5\\\": ErrImagePull: Error response from daemon: Head \\\"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\\\": no basic auth credentials\"" pod="kube-system/aws-node-s8rqs" podUID="410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"
Dec 02 21:24:54 minikube kubelet[1667]: E1202 21:24:54.253103    1667 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"aws-vpc-cni-init\" with ImagePullBackOff: \"Back-off pulling image \\\"602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5\\\": ErrImagePull: Error response from daemon: Head \\\"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\\\": no basic auth credentials\"" pod="kube-system/aws-node-s8rqs" podUID="410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"
Dec 02 21:25:06 minikube kubelet[1667]: E1202 21:25:06.153481    1667 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials" image="602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5"
Dec 02 21:25:06 minikube kubelet[1667]: E1202 21:25:06.153553    1667 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials" image="602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5"
Dec 02 21:25:06 minikube kubelet[1667]: E1202 21:25:06.153741    1667 kuberuntime_manager.go:1341] "Unhandled Error" err="init container &Container{Name:aws-vpc-cni-init,Image:602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni-init:v1.20.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:DISABLE_TCP_EARLY_DEMUX,Value:false,ValueFrom:nil,},EnvVar{Name:ENABLE_IPv6,Value:false,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:cni-bin-dir,ReadOnly:false,MountPath:/host/opt/cni/bin,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-hsgsx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod aws-node-s8rqs_kube-system(410733f0-3b24-4a0f-8eb5-9ed9ce66cc86): ErrImagePull: Error response from daemon: Head \"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\": no basic auth credentials" logger="UnhandledError"
Dec 02 21:25:06 minikube kubelet[1667]: E1202 21:25:06.155042    1667 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"aws-vpc-cni-init\" with ErrImagePull: \"Error response from daemon: Head \\\"https://602401143452.dkr.ecr.us-west-2.amazonaws.com/v2/amazon-k8s-cni-init/manifests/v1.20.5\\\": no basic auth credentials\"" pod="kube-system/aws-node-s8rqs" podUID="410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.167165    1667 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-xtables-lock\") pod \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\" (UID: \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\") "
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.167287    1667 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-xtables-lock" (OuterVolumeSpecName: "xtables-lock") pod "410733f0-3b24-4a0f-8eb5-9ed9ce66cc86" (UID: "410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"). InnerVolumeSpecName "xtables-lock". PluginName "kubernetes.io/host-path", VolumeGIDValue ""
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.167342    1667 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-run-dir" (OuterVolumeSpecName: "run-dir") pod "410733f0-3b24-4a0f-8eb5-9ed9ce66cc86" (UID: "410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"). InnerVolumeSpecName "run-dir". PluginName "kubernetes.io/host-path", VolumeGIDValue ""
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.167244    1667 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"run-dir\" (UniqueName: \"kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-run-dir\") pod \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\" (UID: \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\") "
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.167417    1667 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"log-dir\" (UniqueName: \"kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-log-dir\") pod \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\" (UID: \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\") "
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.167437    1667 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"cni-bin-dir\" (UniqueName: \"kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-cni-bin-dir\") pod \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\" (UID: \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\") "
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.167463    1667 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-log-dir" (OuterVolumeSpecName: "log-dir") pod "410733f0-3b24-4a0f-8eb5-9ed9ce66cc86" (UID: "410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"). InnerVolumeSpecName "log-dir". PluginName "kubernetes.io/host-path", VolumeGIDValue ""
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.167455    1667 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-hsgsx\" (UniqueName: \"kubernetes.io/projected/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-kube-api-access-hsgsx\") pod \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\" (UID: \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\") "
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.167496    1667 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-cni-bin-dir" (OuterVolumeSpecName: "cni-bin-dir") pod "410733f0-3b24-4a0f-8eb5-9ed9ce66cc86" (UID: "410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"). InnerVolumeSpecName "cni-bin-dir". PluginName "kubernetes.io/host-path", VolumeGIDValue ""
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.167544    1667 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"cni-net-dir\" (UniqueName: \"kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-cni-net-dir\") pod \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\" (UID: \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\") "
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.167560    1667 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"bpf-pin-path\" (UniqueName: \"kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-bpf-pin-path\") pod \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\" (UID: \"410733f0-3b24-4a0f-8eb5-9ed9ce66cc86\") "
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.167601    1667 reconciler_common.go:299] "Volume detached for volume \"run-dir\" (UniqueName: \"kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-run-dir\") on node \"minikube\" DevicePath \"\""
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.167609    1667 reconciler_common.go:299] "Volume detached for volume \"log-dir\" (UniqueName: \"kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-log-dir\") on node \"minikube\" DevicePath \"\""
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.167616    1667 reconciler_common.go:299] "Volume detached for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-xtables-lock\") on node \"minikube\" DevicePath \"\""
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.167623    1667 reconciler_common.go:299] "Volume detached for volume \"cni-bin-dir\" (UniqueName: \"kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-cni-bin-dir\") on node \"minikube\" DevicePath \"\""
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.167642    1667 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-bpf-pin-path" (OuterVolumeSpecName: "bpf-pin-path") pod "410733f0-3b24-4a0f-8eb5-9ed9ce66cc86" (UID: "410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"). InnerVolumeSpecName "bpf-pin-path". PluginName "kubernetes.io/host-path", VolumeGIDValue ""
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.167645    1667 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-cni-net-dir" (OuterVolumeSpecName: "cni-net-dir") pod "410733f0-3b24-4a0f-8eb5-9ed9ce66cc86" (UID: "410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"). InnerVolumeSpecName "cni-net-dir". PluginName "kubernetes.io/host-path", VolumeGIDValue ""
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.169189    1667 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-kube-api-access-hsgsx" (OuterVolumeSpecName: "kube-api-access-hsgsx") pod "410733f0-3b24-4a0f-8eb5-9ed9ce66cc86" (UID: "410733f0-3b24-4a0f-8eb5-9ed9ce66cc86"). InnerVolumeSpecName "kube-api-access-hsgsx". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.268805    1667 reconciler_common.go:299] "Volume detached for volume \"cni-net-dir\" (UniqueName: \"kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-cni-net-dir\") on node \"minikube\" DevicePath \"\""
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.268846    1667 reconciler_common.go:299] "Volume detached for volume \"bpf-pin-path\" (UniqueName: \"kubernetes.io/host-path/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-bpf-pin-path\") on node \"minikube\" DevicePath \"\""
Dec 02 21:25:07 minikube kubelet[1667]: I1202 21:25:07.268857    1667 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-hsgsx\" (UniqueName: \"kubernetes.io/projected/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86-kube-api-access-hsgsx\") on node \"minikube\" DevicePath \"\""
Dec 02 21:25:09 minikube kubelet[1667]: I1202 21:25:09.258110    1667 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="410733f0-3b24-4a0f-8eb5-9ed9ce66cc86" path="/var/lib/kubelet/pods/410733f0-3b24-4a0f-8eb5-9ed9ce66cc86/volumes"


==> storage-provisioner [9007c0048dc1] <==
I1202 14:55:49.909624       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1202 14:56:11.121427       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused


==> storage-provisioner [d18907b60bc4] <==
I1202 14:56:26.934251       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1202 14:56:26.995767       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1202 14:56:26.997340       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1202 14:56:44.429631       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1202 14:56:44.429881       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"8be1273d-cb12-4f19-a8cd-3a9627792530", APIVersion:"v1", ResourceVersion:"13760", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_bfcbfa09-5669-4307-915a-3ea2218caa80 became leader
I1202 14:56:44.430019       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_bfcbfa09-5669-4307-915a-3ea2218caa80!
I1202 14:56:44.532379       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_bfcbfa09-5669-4307-915a-3ea2218caa80!

